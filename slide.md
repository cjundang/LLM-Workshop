---
marp: true
title: "From Text to AI Assistant"
theme: default
paginate: true
_class: lead
backgroundImage: url('images/background.jpg')
backgroundSize: cover
backgroundColor: "#ffffff"
color: "#003366"
header: "Assist. Prof. Dr. Chanankorn Jandaeng | Walailak University"
footer: "From Text to AI Assistant"
---

# From Text to AI Assistant Using Machine Learning and RAG Framework
## ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏π‡πà‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞‡πÇ‡∏î‡∏¢‡∏≠‡∏≤‡∏®‡∏±‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ç‡∏≠‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏Å‡∏£‡∏≠‡∏ö‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î RAG
### 4-6 ‡∏ï‡∏∏‡∏•‡∏≤‡∏Ñ‡∏° 2568
### ‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏ß‡∏¥‡∏ä‡∏≤‡∏™‡∏≤‡∏£‡∏™‡∏ô‡πÄ‡∏ó‡∏®‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå ‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏ß‡∏•‡∏±‡∏¢‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå

---
# ü§ñ 01-Artificial Ingelligence 
## ‡∏ú‡∏®. ‡∏î‡∏£. ‡∏ä‡∏ô‡∏±‡∏ô‡∏ó‡πå‡∏Å‡∏£‡∏ì‡πå ‡∏à‡∏±‡∏ô‡πÅ‡∏î‡∏á
### ‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏ß‡∏¥‡∏ä‡∏≤‡∏™‡∏≤‡∏£‡∏™‡∏ô‡πÄ‡∏ó‡∏®‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå ‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏ß‡∏•‡∏±‡∏¢‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå

---

## 1. ‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏õ‡∏£‡∏∞‡∏î‡∏¥‡∏©‡∏ê‡πå (Artificial Intelligence)

**AI ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?**  
‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡πÉ‡∏´‡πâ ‚Äú‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‚Äù ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÇ‡∏î‡∏¢‡∏õ‡∏Å‡∏ï‡∏¥ **‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏™‡∏ï‡∏¥‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏Ç‡∏≠‡∏á‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå**

‡πÄ‡∏ä‡πà‡∏ô  
- ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ (Learning)  
- ‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏• (Reasoning)  
- ‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ (Problem Solving)  
- ‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏†‡∏≤‡∏©‡∏≤ (Language)

---

### üîπ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÉ‡∏ô‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡∏à‡∏£‡∏¥‡∏á

- ‡∏£‡∏ñ‡∏¢‡∏ô‡∏ï‡πå‡πÑ‡∏£‡πâ‡∏Ñ‡∏ô‡∏Ç‡∏±‡∏ö üöó  
- Chatbot ‡πÅ‡∏•‡∏∞‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏™‡∏°‡∏∑‡∏≠‡∏ô ‡πÄ‡∏ä‡πà‡∏ô Siri  
- ‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÉ‡∏ô Shopee ‡∏´‡∏£‡∏∑‡∏≠ Netflix  

---

### üîπ ‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏±‡πâ‡∏ô

> AI ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏±‡∏Å‡∏£ ‚Äú‡∏Ñ‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÑ‡∏î‡πâ‚Äù ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå  
> ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏™‡∏°‡∏±‡∏¢‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏∏‡∏Å‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ

---

## 2. Machine Learning (ML)

**Machine Learning ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?**  
‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå ‚Äú‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‚Äù ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏±‡πà‡∏á‡∏ó‡∏∏‡∏Å‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

---

### üîπ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢

- ‡∏ù‡∏∂‡∏Å‡πÉ‡∏´‡πâ‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏à‡∏≥‡πÅ‡∏ô‡∏Å ‚Äú‡∏≠‡∏µ‡πÄ‡∏°‡∏•‡∏Ç‡∏¢‡∏∞‚Äù  
- ‡∏™‡∏≠‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏´‡∏ô‡∏±‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ä‡∏≠‡∏ö‡∏ö‡∏ô Netflix  
- ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠ ‚Äú‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏¢‡∏≠‡∏î‡∏Ç‡∏≤‡∏¢‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï‚Äù

---

### üîπ ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡∏≠‡∏á Machine Learning

1. **Supervised Learning** ‚Äì ‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏Å‡∏≥‡∏Å‡∏±‡∏ö (‡∏°‡∏µ label)  
2. **Unsupervised Learning** ‚Äì ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö ‡πÉ‡∏´‡πâ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÄ‡∏≠‡∏á  
3. **Reinforcement Learning** ‚Äì ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏£‡∏≤‡∏á‡∏ß‡∏±‡∏•/‡∏ö‡∏ó‡∏•‡∏á‡πÇ‡∏ó‡∏©  

---

### üîπ ‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏±‡πâ‡∏ô

> ML ‡∏Ñ‡∏∑‡∏≠‡∏™‡∏∞‡∏û‡∏≤‡∏ô‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á ‚Äú‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏Ç‡∏≠‡∏á AI‚Äù  
> ‡∏Å‡∏±‡∏ö ‚Äú‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏ß‡∏±‡∏ô‚Äù

---
![w:600px](images/traditional_vs_ML.png)

---
## Machine Learning Process
### 1. Initial Dataset

‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏∏‡∏î‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö (Raw Data) ‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏´‡∏•‡πà‡∏á ‡πÄ‡∏ä‡πà‡∏ô ‡πÑ‡∏ü‡∏•‡πå CSV, ‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•, ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ã‡∏ô‡πÄ‡∏ã‡∏≠‡∏£‡πå ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏µ‡πâ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• ML ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô ‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
![w:600px](images/ml_process_01.png)

---
### 2. Exploratory Data Analysis (EDA)

‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏≥‡∏£‡∏ß‡∏à‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û ‡πÄ‡∏ä‡πà‡∏ô

* ‡∏Å‡∏≤‡∏£‡∏î‡∏π‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£
* ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö outlier
* ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ **PCA (Principal Component Analysis)** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏°‡∏¥‡∏ï‡∏¥
* ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ **SOM (Self-Organizing Map)** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤ pattern

![w:600px](images/ml_process_01.png)

---
### 3. Data Cleaning / Pre-processing

‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° ‡πÄ‡∏ä‡πà‡∏ô

* ‡∏•‡∏ö‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î‡∏´‡∏≤‡∏¢‡πÑ‡∏õ
* ‡πÅ‡∏õ‡∏•‡∏á categorical ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
* ‡∏•‡∏ö‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô

![w:600px](images/ml_process_01.png)
---
### 4. Data Splitting

‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô 2 ‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏Å:

* **Training Set (80%)** ‚Üí ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
* **Test Set (20%)** ‚Üí ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
![w:600px](images/ml_process_02.png)

---
### 5. Learning Algorithms

‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° ‡πÄ‡∏ä‡πà‡∏ô **SVM (Support Vector Machine)**
**DL (Deep Learning)**  **GBM (Gradient Boosting Machine)** **RF (Random Forest)** **kNN (k-Nearest Neighbors)**
![w:600px](images/ml_process_02.png)

---
### 6. Hyperparameter Optimization

‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡πÄ‡∏ä‡πà‡∏ô ‡∏Ñ‡πà‡∏≤ k ‡∏Ç‡∏≠‡∏á kNN, ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô tree ‡∏Ç‡∏≠‡∏á Random Forest ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
![w:600px](images/ml_process_02.png)

---
### 7. Feature Selection

‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏∏‡∏ì‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÑ‡∏î‡πâ‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡∏•‡∏î Overfitting
![w:600px](images/ml_process_02.png)

---
### 8. Cross-Validation & Training Model

* ‡πÉ‡∏ä‡πâ **Cross-Validation** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•
* ‡∏ù‡∏∂‡∏Å (train) ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ö‡∏ô training set ‡πÅ‡∏•‡∏∞‡πÑ‡∏î‡πâ **Trained Model**
![w:600px](images/ml_process_03.png)

---
### 9. Evaluate Model Performance

‡∏ß‡∏±‡∏î‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ metric ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° ‡πÄ‡∏ä‡πà‡∏ô

* **Classification:** Accuracy, Precision, Recall, F1-score, MCC
* **Regression:** RMSE, MSE, ($R^2$)
![w:600px](images/ml_process_03.png)

---
### 10. Predicted Values

‡∏ô‡∏≥‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏õ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ñ‡πà‡∏≤‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà

---
 
## 3. Deep Learning (DL)

**Deep Learning ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?**  
‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ **‡πÇ‡∏Ñ‡∏£‡∏á‡∏Ç‡πà‡∏≤‡∏¢‡∏õ‡∏£‡∏∞‡∏™‡∏≤‡∏ó‡πÄ‡∏ó‡∏µ‡∏¢‡∏° (Neural Networks)**  
‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏µ‡∏¢‡∏ô‡πÅ‡∏ö‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏™‡∏°‡∏≠‡∏á‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå

---

### üîπ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏´‡πá‡∏ô‡∏ö‡πà‡∏≠‡∏¢

- ‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡∏†‡∏≤‡∏û‡πÅ‡∏°‡∏ß üê± ‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏∏‡∏ô‡∏±‡∏Ç üê∂  
- ‡∏Å‡∏≤‡∏£‡∏£‡∏π‡πâ‡∏à‡∏≥‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏û‡∏π‡∏î (Siri, Google Voice)  
- ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏£‡∏≤‡∏Ñ‡∏≤‡∏´‡∏∏‡πâ‡∏ô üìà  

---

### üîπ ‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ç‡∏≠‡∏á Deep Learning

- ‡∏ó‡∏≥‡πÑ‡∏î‡πâ‡∏î‡∏µ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ‚Äú‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ù‡∏∂‡∏Å‡∏°‡∏≤‚Äù  
- ‡πÑ‡∏°‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à ‚Äú‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏†‡∏≤‡∏©‡∏≤‚Äù ‡∏à‡∏£‡∏¥‡∏á ‡πÜ  
- ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å  

---

### üîπ Key Takeaway

> Deep Learning ‡∏Ñ‡∏∑‡∏≠‡∏£‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á AI  
> ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á ‚Äú‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏†‡∏≤‡∏©‡∏≤‚Äù ‡πÑ‡∏î‡πâ‡πÑ‡∏°‡πà‡∏î‡∏µ‡∏û‡∏≠  
> ‡∏à‡∏∂‡∏á‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÑ‡∏õ‡∏™‡∏π‡πà **LLM**

---

## 4. ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á Language Model

### üîπ ‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô

LLM ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ **‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏≥**  
‡πÄ‡∏û‡∏∑‡πà‡∏≠ ‚Äú‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‚Äù ‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:**  
> ‚Äú‡∏â‡∏±‡∏ô‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô ___‚Äù  
> ‚Üí ‚Äú‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‚Äù, ‚Äú‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‚Äù, ‚Äú‡∏à‡∏î‡∏´‡∏°‡∏≤‡∏¢‚Äù

---

### üîπ ‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç

1. **Tokenization:** ‡πÅ‡∏ö‡πà‡∏á‡∏Ñ‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏¥‡πâ‡∏ô‡πÄ‡∏•‡πá‡∏Å ‡πÜ  
   - ‚Äú‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö‚Äù ‚Üí ["‡∏™‡∏ß‡∏±", "‡∏™‡∏î‡∏µ", "‡∏Ñ‡∏£‡∏±‡∏ö"]  

2. **Embeddings:** ‡πÅ‡∏õ‡∏•‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ token ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç  
   - ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• ‚Äú‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‚Äù ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡πÑ‡∏î‡πâ  

---

## 5. Large Language Model (LLM)

**LLM ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?**  
‡πÇ‡∏°‡πÄ‡∏î‡∏• AI ‡∏ó‡∏µ‡πà ‚Äú‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏î‡πâ‚Äù  
‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏° **Transformer**

---

### üîπ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á LLM ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å

- **ChatGPT** üß†  
- **Google Bard (Gemini)** üåê  
- **Claude (Anthropic)**  
- **LLaMA (Meta)**  

---

### üîπ ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á LLM

- ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÅ‡∏•‡∏∞‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‚úçÔ∏è  
- ‡πÅ‡∏õ‡∏•‡∏†‡∏≤‡∏©‡∏≤ üåè  
- ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° üó£Ô∏è  
- ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î üíª  

---

### üîπ Key Point ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô ‡∏°.‡∏õ‡∏•‡∏≤‡∏¢

- LLM ‚Äú‡∏≠‡πà‡∏≤‡∏ô-‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô-‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‚Äù ‡∏†‡∏≤‡∏©‡∏≤‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå  
- ‡πÉ‡∏ä‡πâ ‚ÄúPrompt‚Äù ‡∏ö‡∏≠‡∏Å‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£  
- ‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏´‡∏•‡∏≤‡∏¢‡∏á‡∏≤‡∏ô ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà  

---

## 6. LLM ‡∏ï‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å Deep Learning Model ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£

| ‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô | Deep Learning ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ | LLM |
|----------|--------------------|-----|
| **‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ù‡∏∂‡∏Å** | ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏á‡∏≤‡∏ô | ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏´‡∏≤‡∏®‡∏≤‡∏•‡∏´‡∏•‡∏≤‡∏¢‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ |
| **‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå** | ‡∏á‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ‡πÄ‡∏ä‡πà‡∏ô ‡∏à‡∏≥‡∏†‡∏≤‡∏û | ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏≤‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÅ‡∏ö‡∏ö‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ |
| **‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ** | ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏Ñ‡∏ö ‡πÜ | ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Ç‡∏≠‡∏á‡∏†‡∏≤‡∏©‡∏≤ |
| **‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô** | ‡∏ï‡πâ‡∏≠‡∏á‡∏ù‡∏∂‡∏Å‡πÉ‡∏´‡∏°‡πà‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏á‡∏≤‡∏ô | ‡πÉ‡∏ä‡πâ Prompt ‡πÑ‡∏î‡πâ‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö |
| **‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•** | ResNet, RNN, LSTM | GPT, LLaMA, PaLM |

---

## üéØ ‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢

- AI ‚Üí ‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏Å‡∏ß‡πâ‡∏≤‡∏á ‚Äú‡πÉ‡∏´‡πâ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Ñ‡∏¥‡∏î‡πÑ‡∏î‡πâ‚Äù  
- ML ‚Üí ‡πÉ‡∏´‡πâ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á ‚Äú‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‚Äù  
- DL ‚Üí ‡πÉ‡∏ä‡πâ‡∏™‡∏°‡∏≠‡∏á‡∏à‡∏≥‡∏•‡∏≠‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏•‡∏∂‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô  
- LLM ‚Üí ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏†‡∏≤‡∏©‡∏≤‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏î‡πâ  

‚ú® ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ó‡∏µ‡πà ChatGPT ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ ‚Äú‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‚Äù ‡∏Å‡∏±‡∏ö‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå ‚ú®

---

# üß† 02 - ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏¥‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥  
### (Statistical Text Analysis)

**Chanankorn**  
*School of Informatics, Walailak University*  

---

## üîç ‡∏ö‡∏ó‡∏ô‡∏≥‡πÄ‡∏ä‡∏¥‡∏á‡∏ó‡∏§‡∏©‡∏é‡∏µ

‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏¥‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≤‡∏á  **Data Science** ‡πÅ‡∏•‡∏∞ **Computational Linguistics**  ‡∏ó‡∏µ‡πà‡∏ô‡∏≥‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≤‡∏á **‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡πÅ‡∏•‡∏∞‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥**  ‡∏°‡∏≤‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡∏Å‡∏±‡∏ö **‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (Text Data)**

üéØ ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏Å:
- ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÄ‡∏ä‡∏¥‡∏á‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì
- ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó, ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠, ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå ‡∏Ø‡∏•‡∏Ø

---

## 1Ô∏è‚É£ Text Preprocessing  
### (‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°)

‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î  
‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ **Noise** ‡πÄ‡∏ä‡πà‡∏ô ‡∏Ñ‡∏≥‡∏ü‡∏∏‡πà‡∏°‡πÄ‡∏ü‡∏∑‡∏≠‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ß‡∏£‡∏£‡∏Ñ‡∏ï‡∏≠‡∏ô

---

### üîπ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á Text Preprocessing

1. **Tokenization** ‚Äì ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥  
2. **Stopword Removal** ‚Äì ‡∏•‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç  
3. **Stemming / Lemmatization** ‚Äì ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡∏≥‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô  
4. **Symbol & Number Removal** ‚Äì ‡∏•‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÅ‡∏•‡∏∞‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô  

---

### ‚úÇÔ∏è Tokenization ‚Äì ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥

| ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á | ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏´‡∏•‡∏±‡∏á Tokenization |
| -------- | -------------------------- |
| ‚ÄúData scientists are analyzing large amounts of data in 2024!‚Äù | `['Data', 'scientists', 'are', 'analyzing', 'large', ...]` |

üìò ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏´‡∏£‡∏∑‡∏≠‡∏ß‡∏•‡∏µ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ô‡∏≥‡πÑ‡∏õ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÑ‡∏î‡πâ

---

### üö´ Stopword Removal ‚Äì ‡∏•‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç

‡∏•‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ ‡πÄ‡∏ä‡πà‡∏ô ‚Äúthe‚Äù, ‚Äúof‚Äù, ‚Äúin‚Äù  

| ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á | ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå |
| -------- | -------- |
| ‚ÄúData scientists are analyzing large amounts of data in 2024!‚Äù | `['Data', 'scientists', 'analyzing', 'large', 'data']` |

---

### üî§ Lemmatization ‚Äì ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô

‡∏£‡∏ß‡∏°‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏£‡∏≤‡∏Å‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô ‡πÄ‡∏ä‡πà‡∏ô  
‚Äúanalyzing‚Äù ‚Üí ‚Äúanalyze‚Äù, ‚Äúmodels‚Äù ‚Üí ‚Äúmodel‚Äù

---

### üî£ Symbol & Number Removal

‡∏•‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢ ‡πÅ‡∏•‡∏∞‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô  

üìç ‚ÄúAI model perform exceptionally well experiment‚Äù

---

### üß© ‡∏™‡∏£‡∏∏‡∏õ‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

| ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô | ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå | ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå |
| -------- | ------------- | -------- |
| Tokenization | ‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥ | ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥ |
| Stopword Removal | ‡∏•‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ | ‡∏•‡∏î Noise |
| Lemmatization | ‡∏£‡∏ß‡∏°‡∏£‡∏π‡∏õ‡∏Ñ‡∏≥ | ‡∏Ñ‡∏≥‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô |
| Symbol Removal | ‡∏•‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç/‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå | ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î |

---

### üíª ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î Python

```python
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import string

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

text = "Statistical text analysis involves processing text data."

tokens = word_tokenize(text.lower())
filtered = [w for w in tokens if w not in stopwords.words('english') and w not in string.punctuation]
lemmatizer = WordNetLemmatizer()
lemmatized = [lemmatizer.lemmatize(word) for word in filtered]

print("After Cleaning:", lemmatized)
````

---

## 2Ô∏è‚É£ Statistical Representation

### (‡∏Å‡∏≤‡∏£‡πÅ‡∏ó‡∏ô‡∏Ñ‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏¥‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥)

‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡πâ‡∏ß ‚Üí ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏ó‡∏ô‡∏Ñ‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ó‡∏≤‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÑ‡∏î‡πâ

---

### üîπ Bag of Words (BoW)

**‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î:**
‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏™‡∏ô‡πÉ‡∏à‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏≥

üìä ‡πÄ‡∏ä‡πà‡∏ô
‚Äúdata scientist analyze large amount data‚Äù ‚Üí `[data:2, scientist:1, ...]`

‚úÖ ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢
‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏ô‡πÉ‡∏à‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥

---

### üîπ TF‚ÄìIDF (Term Frequency ‚Äì Inverse Document Frequency)

**‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î:**
‡πÉ‡∏´‡πâ‡∏Ñ‡πà‡∏≤‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÉ‡∏ô‡∏ö‡∏≤‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£

üßÆ ‡∏™‡∏π‡∏ï‡∏£:
$TF-IDF(t, d) = TF(t, d) √ó log(N / n_t)$

‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÇ‡∏î‡∏î‡πÄ‡∏î‡πà‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô ‡πÄ‡∏ä‡πà‡∏ô

* ‚Äúexperiment‚Äù ‚Üí ‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á
* ‚Äúdata‚Äù ‚Üí ‡∏Ñ‡πà‡∏≤‡∏ï‡πà‡∏≥

---

### üîπ N-grams

**‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î:**
‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏Ñ‡∏≥‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏Å‡∏±‡∏ô (sequence of words)

* Unigram ‚Üí ‚Äúmachine‚Äù
* Bigram ‚Üí ‚Äúmachine learning‚Äù
* Trigram ‚Üí ‚Äúmachine learning helps‚Äù

‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏à‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤ BoW

---

### üß© ‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏Å‡∏≤‡∏£‡πÅ‡∏ó‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°

| ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ  | ‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£             | ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ       | ‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î       |
| ------- | ------------------- | ----------- | -------------- |
| BoW     | ‡∏ô‡∏±‡∏ö‡∏Ñ‡∏≥               | ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢  | ‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏≥  |
| TF‚ÄìIDF  | ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç | ‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç | ‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó    |
| N-grams | ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏≥      | ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ö‡∏£‡∏¥‡∏ö‡∏ó | ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏¢‡∏≤‡∏¢‡πÉ‡∏´‡∏ç‡πà |

---

## 3Ô∏è‚É£ Descriptive Statistics

### (‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏û‡∏£‡∏£‡∏ì‡∏ô‡∏≤)

‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‡πÄ‡∏ä‡πà‡∏ô

* ‡∏ô‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢
* ‡∏™‡∏£‡πâ‡∏≤‡∏á **Word Cloud**

---

### üíª ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î

```python
from collections import Counter
from wordcloud import WordCloud
import matplotlib.pyplot as plt

words = ["data", "text", "analysis", "data", "machine", "learning"]
freq = Counter(words)
print(freq.most_common(5))

wc = WordCloud(width=800, height=400, background_color='white').generate(" ".join(words))
plt.imshow(wc, interpolation='bilinear')
plt.axis("off")
plt.show()
```

---

## 4Ô∏è‚É£ Inferential & Predictive Analysis

### üîπ Topic Modeling (LDA)

‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÇ‡∏î‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥

```python
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer

docs = ["Machine learning improves analytics.", "Deep learning in AI."]
X = CountVectorizer().fit_transform(docs)
lda = LatentDirichletAllocation(n_components=2).fit(X)
```

---

### üîπ Sentiment Analysis

‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‡πÄ‡∏ä‡πà‡∏ô ‡∏ö‡∏ß‡∏Å / ‡∏Å‡∏•‡∏≤‡∏á / ‡∏•‡∏ö

```python
from textblob import TextBlob
texts = ["I love this product!", "Worst experience ever."]
for t in texts:
    print(t, TextBlob(t).sentiment.polarity)
```

---

### üîπ Clustering / Classification

‡πÉ‡∏ä‡πâ K-means ‡∏´‡∏£‡∏∑‡∏≠ Na√Øve Bayes
‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥

```python
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import TfidfVectorizer
texts = ["AI and ML", "Statistical data science"]
X = TfidfVectorizer().fit_transform(texts)
KMeans(n_clusters=2).fit(X)
```

---

## 5Ô∏è‚É£ ‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡πÄ‡∏ä‡∏¥‡∏á‡∏ó‡∏§‡∏©‡∏é‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á

| ‡∏™‡∏≤‡∏Ç‡∏≤‡∏ß‡∏¥‡∏ä‡∏≤                              | ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢                           |
| ------------------------------------- | ---------------------------------- |
| **Corpus Linguistics**                | ‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏†‡∏≤‡∏©‡∏≤‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ñ‡∏•‡∏±‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà  |
| **Information Retrieval (IR)**        | ‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á |
| **Natural Language Processing (NLP)** | ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏†‡∏≤‡∏©‡∏≤‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå     |

---

## 6Ô∏è‚É£ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå

| ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå                | ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢                    |
| -------------------------- | --------------------------- |
| ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÇ‡∏û‡∏™‡∏ï‡πå‡∏™‡∏±‡∏á‡∏Ñ‡∏°‡∏≠‡∏≠‡∏ô‡πÑ‡∏•‡∏ô‡πå | ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ      |
| ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à      | ‡∏´‡∏≤‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏à‡∏≤‡∏Å‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°    |
| ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏£‡∏µ‡∏ß‡∏¥‡∏ß‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤       | ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏∂‡∏á‡∏û‡∏≠‡πÉ‡∏à‡∏Ç‡∏≠‡∏á‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤ |
| ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£     | ‡∏î‡∏∂‡∏á‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏î‡πâ‡∏ß‡∏¢ TF-IDF       |

---

## 7Ô∏è‚É£ ‡∏Å‡∏£‡∏ì‡∏µ‡∏®‡∏∂‡∏Å‡∏©‡∏≤: ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö Spam

‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô **Spam / Ham**
‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ TF‚ÄìIDF ‡πÅ‡∏•‡∏∞ Na√Øve Bayes

---

### üíª ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡∏¢‡πà‡∏≠

```python
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer

X_train, X_test, y_train, y_test = train_test_split(texts, labels)
tfidf = TfidfVectorizer()
X_train_tfidf = tfidf.fit_transform(X_train)

model = MultinomialNB().fit(X_train_tfidf, y_train)
```

---

### üîé ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á Spam Detection

| ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô        | ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î                 |
| -------------- | -------------------------- |
| ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•     | Spam / Ham messages        |
| Preprocessing  | ‡∏•‡∏ö stopwords, lowercase    |
| TF‚ÄìIDF         | ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç      |
| Classification | Na√Øve Bayes                |
| Evaluation     | Accuracy, Confusion Matrix |

---

## üß† ‡∏Å‡∏≤‡∏£‡∏Ç‡∏¢‡∏≤‡∏¢‡πÄ‡∏ä‡∏¥‡∏á‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£

* ‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡∏∑‡πà‡∏ô: Logistic Regression, SVM, Random Forest
* ‡πÉ‡∏ä‡πâ **N-grams** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ö‡∏£‡∏¥‡∏ö‡∏ó
* ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå **Feature Importance**
* ‡πÉ‡∏ä‡πâ‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á ‡πÄ‡∏ä‡πà‡∏ô *Kaggle SMS Spam Dataset*

---

# üìò ‡∏™‡∏£‡∏∏‡∏õ

* Text Analysis ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
* ‡πÉ‡∏ä‡πâ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÅ‡∏•‡∏∞ Machine Learning ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
* ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏Ç‡∏ß‡∏≤‡∏á‡πÉ‡∏ô Data Science ‡πÅ‡∏•‡∏∞ NLP

---

# üìñ 03 - Contextual Text Analysis
## ‡∏ú‡∏®. ‡∏î‡∏£. ‡∏ä‡∏ô‡∏±‡∏ô‡∏ó‡πå‡∏Å‡∏£‡∏ì‡πå ‡∏à‡∏±‡∏ô‡πÅ‡∏î‡∏á
### ‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏ß‡∏¥‡∏ä‡∏≤‡∏™‡∏≤‡∏£‡∏™‡∏ô‡πÄ‡∏ó‡∏®‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå ‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏ß‡∏•‡∏±‡∏¢‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå
---

# üß≠ ‡∏ö‡∏ó‡∏ô‡∏≥: ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏¥‡∏á‡∏ö‡∏£‡∏¥‡∏ö‡∏ó

**‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏´‡∏•‡∏±‡∏Å:**  
‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°" ‡πÇ‡∏î‡∏¢‡∏Ñ‡∏≥‡∏ô‡∏∂‡∏á‡∏ñ‡∏∂‡∏á‡∏ö‡∏£‡∏¥‡∏ö‡∏ó (Context)

> ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ ‚Äúbank‚Äù ‡∏≠‡∏≤‡∏à‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á ‚Äú‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‚Äù ‡∏´‡∏£‡∏∑‡∏≠ ‚Äú‡∏£‡∏¥‡∏°‡∏ù‡∏±‡πà‡∏á‡∏ô‡πâ‡∏≥‚Äù  
> ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏à‡∏∂‡∏á‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°

---

# ‚öôÔ∏è Text Embedding ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£

**Text Embedding**: ‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏°‡∏¥‡∏ï‡∏¥‡∏™‡∏π‡∏á  
‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô ‚Üí ‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏Å‡∏•‡πâ‡∏Å‡∏±‡∏ô

- Cosine Similarity ‚Üí ‡∏ß‡∏±‡∏î‡∏°‡∏∏‡∏°‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå  
- Euclidean Distance ‚Üí ‡∏ß‡∏±‡∏î‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå

> ‚Äú‡∏Ñ‡∏£‡∏π‚Äù ‡πÉ‡∏Å‡∏•‡πâ‡∏Å‡∏±‡∏ö ‚Äú‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå‚Äù ‡πÅ‡∏•‡∏∞ ‚Äú‡∏ú‡∏π‡πâ‡∏™‡∏≠‡∏ô‚Äù

---

# üß© ‡∏ó‡∏§‡∏©‡∏é‡∏µ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏ó‡∏≤‡∏á‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£

## Distributional Semantics Theory
> ‚ÄúYou shall know a word by the company it keeps.‚Äù  
(Firth, 1957)

- ‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏°‡∏±‡∏Å‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô
- ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏ä‡∏¥‡∏á‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢

---

## Vector Space Model (VSM)

- ‡∏Å‡∏≤‡∏£‡πÅ‡∏ó‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå  
- ‡∏ß‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Ñ‡∏•‡∏∂‡∏á‡∏î‡πâ‡∏ß‡∏¢ cosine / distance  
- ‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏∏‡∏î‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Ç‡∏≠‡∏á‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î Embedding

---

## Word Embedding Models

| ‡πÇ‡∏°‡πÄ‡∏î‡∏• | ‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î | ‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô |
|-------|--------|---------|
| **Word2Vec** | ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ñ‡∏≥‡∏à‡∏≤‡∏Å‡∏ö‡∏£‡∏¥‡∏ö‡∏ó | ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢ |
| **GloVe** | ‡πÉ‡∏ä‡πâ Co-occurrence Statistics | ‡πÄ‡∏ô‡πâ‡∏ô global context |
| **FastText** | ‡πÉ‡∏ä‡πâ subword | ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡πÉ‡∏´‡∏°‡πà (OOV words) |

---

## Contextual Embeddings

- ‡πÉ‡∏ä‡πâ Deep Neural Networks (LSTM / Transformer)  
- ‡∏Ñ‡∏≥‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏°‡∏µ‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡πÉ‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô

| ‡πÇ‡∏°‡πÄ‡∏î‡∏• | ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞ | ‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô |
|-------|---------|---------|
| ELMo | Bi-LSTM | ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏™‡∏≠‡∏á‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á |
| BERT | Transformer ‡∏™‡∏≠‡∏á‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á | ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏à‡∏≤‡∏Å‡∏ã‡πâ‡∏≤‡∏¢-‡∏Ç‡∏ß‡∏≤ |
| GPT | Transformer ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß | ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á |
| SBERT | ‡∏õ‡∏£‡∏±‡∏ö BERT ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ | ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô Semantic Search |

---

# üî¨ ‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏ó‡∏§‡∏©‡∏é‡∏µ‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á

## Manifold Hypothesis
- ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡πÉ‡∏ô‡∏°‡∏¥‡∏ï‡∏¥‡∏™‡∏π‡∏á  
- Embedding ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏¢‡πÉ‡∏ô (semantic manifold)

## Transfer Learning
- ‡πÉ‡∏ä‡πâ embedding ‡∏ó‡∏µ‡πà‡∏ù‡∏∂‡∏Å‡∏°‡∏≤‡∏Å‡πà‡∏≠‡∏ô ‡πÄ‡∏ä‡πà‡∏ô BERT, GloVe  
- ‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ fine-tuning ‡πÉ‡∏ô‡∏á‡∏≤‡∏ô‡πÉ‡∏´‡∏°‡πà (‡πÄ‡∏ä‡πà‡∏ô Sentiment Analysis)

## Explainability
- ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏Å‡∏ô‡∏Ç‡∏≠‡∏á‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå ‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏û‡∏®, ‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå

---

# üíº ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ Text Embedding

| ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå | ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ |
|--------------|----------|
| Semantic Search | ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ï‡∏£‡∏á‡∏ï‡∏±‡∏ß |
| Clustering | ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á |
| Recommendation | ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô |
| Similarity Measurement | ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏•‡∏≠‡∏Å‡∏á‡∏≤‡∏ô / ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ |
| LLM Understanding | ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô GPT, BERT, Claude ‡∏Ø‡∏•‡∏Ø |

---

# üß† ‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏ä‡∏¥‡∏á‡∏ó‡∏§‡∏©‡∏é‡∏µ

1. Embedding = ‡∏™‡∏∞‡∏û‡∏≤‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏†‡∏≤‡∏©‡∏≤‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå  
2. Contextual Embedding = ‡∏ß‡∏¥‡∏ß‡∏±‡∏í‡∏ô‡∏≤‡∏Å‡∏≤‡∏£‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á‡∏Ç‡∏≠‡∏á NLP  
3. ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö Search, Chatbot, ‡πÅ‡∏•‡∏∞ LLMs

---

# üß™ Workshop: Text Embedding

**‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå**  
- ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå  
- ‡∏ß‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Ñ‡∏•‡∏∂‡∏á‡πÄ‡∏ä‡∏¥‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢  
- ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö Keyword vs Contextual Approach

---

## üîß ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

- Python / Google Colab  
- Libraries: `sentence-transformers`, `numpy`, `sklearn`

```python
texts = [
 "‡∏£‡∏ñ‡∏¢‡∏ô‡∏ï‡πå‡πÑ‡∏ü‡∏ü‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏¥‡∏ï‡∏£‡∏ï‡πà‡∏≠‡∏™‡∏¥‡πà‡∏á‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°",
 "‡∏¢‡∏≤‡∏ô‡∏¢‡∏ô‡∏ï‡πå‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏¥‡∏¢‡∏°",
 "‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏¥‡∏ô‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡πâ‡∏≤‡∏ô",
 "‡πÅ‡∏°‡πà‡∏ô‡πâ‡∏≥‡∏°‡∏µ‡∏ô‡πâ‡∏≥‡∏°‡∏≤‡∏Å‡πÉ‡∏ô‡∏§‡∏î‡∏π‡∏ù‡∏ô"
]
```

---

## üß† ‡∏™‡∏£‡πâ‡∏≤‡∏á Sentence Embeddings

```python
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
embeddings = model.encode(texts)
```

- ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‚Üí ‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏°‡∏¥‡∏ï‡∏¥ 384  
- ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡πÉ‡∏Å‡∏•‡πâ‡∏Å‡∏±‡∏ô ‚Üí ‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏Å‡∏•‡πâ‡∏Å‡∏±‡∏ô

---

## üìè Semantic Similarity

```python
from sentence_transformers import util
similarity = util.cos_sim(embeddings, embeddings)
```

| ‡∏Ñ‡πà‡∏≤ | ‡∏Å‡∏≤‡∏£‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏° |
|------|-------------|
| 0.8‚Äì1.0 | ‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô‡∏°‡∏≤‡∏Å |
| 0.5‚Äì0.8 | ‡∏Ñ‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≤‡∏á‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢ |
| <0.2 | ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏°‡∏≤‡∏Å |

---

# üìä Visualization

```python
from sklearn.manifold import TSNE
plt.scatter(...)
```

- ‡∏à‡∏∏‡∏î‡πÉ‡∏Å‡∏•‡πâ‡∏Å‡∏±‡∏ô ‚Üí ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô  
- ‡πÅ‡∏™‡∏î‡∏á semantic clusters

---

# üß™ Showcase: Spam Detection

**‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î:**  
‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô 2 ‡∏Å‡∏•‡∏∏‡πà‡∏° ‚Äî Spam vs Ham  
‡πÉ‡∏ä‡πâ Contextual Embedding + KNN Classifier

```python
data = {...}
model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = model.encode(data['text'])
```

---

## üîç ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå

| ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° | ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå |
|-----------|----------|
| ‚ÄúGet rich quick!‚Äù | spam |
| ‚ÄúReview project report.‚Äù | ham |

**Embedding ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÄ‡∏à‡∏ï‡∏ô‡∏≤ (intent)** ‡πÅ‡∏°‡πâ‡πÑ‡∏°‡πà‡∏°‡∏µ keyword ‚Äúspam‚Äù

---

# üß† Key Insights

- Context ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏ß‡πà‡∏≤‡∏Ñ‡∏≥‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß  
- Embedding ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à ‚Äú‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‚Äù  
- ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á‡πÉ‡∏ô‡∏á‡∏≤‡∏ô Search, Chatbot, ‡πÅ‡∏•‡∏∞ AI Model  
- ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á LLM ‡∏ó‡∏∏‡∏Å‡∏ä‡∏ô‡∏¥‡∏î

---

# ü§ñ Workshop:  
**04-‡∏™‡∏£‡πâ‡∏≤‡∏á AI ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ (Document Q&A)**  
‡∏î‡πâ‡∏ß‡∏¢‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î **RAG Framework**

üë®‚Äçüè´ ‡∏ú‡∏π‡πâ‡∏™‡∏≠‡∏ô: ‡∏î‡∏£. CJ  
Walailak University  

---

# üéØ ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á Workshop

- ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ß‡πà‡∏≤ **AI ‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£**  
- ‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î **RAG ‚Äì Retrieval Augmented Generation**  
- ‡πÑ‡∏î‡πâ‡∏•‡∏≠‡∏á **‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡∏ñ‡∏≤‡∏°-‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏à‡∏£‡∏¥‡∏á**  
- ‡∏™‡∏£‡πâ‡∏≤‡∏á **Chatbot ‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏≠‡πà‡∏≤‡∏ô‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠** ‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á

---

# üß† ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏û‡∏ö

> ‚Äú‡∏≠‡∏¢‡∏≤‡∏Å‡∏£‡∏π‡πâ‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏° ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏≤‡∏Å‡∏≠‡πà‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î!‚Äù

AI ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢...

1. ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÅ‡∏ó‡∏ô‡πÄ‡∏£‡∏≤  
2. ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏™‡πà‡∏ß‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç  
3. ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á

---

# üîç ‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏Ç‡∏≠‡∏á RAG

**RAG = Retrieval + Augmented + Generation**

| ‡∏™‡πà‡∏ß‡∏ô | ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà |
|------|----------|
| Retrieval | ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ |
| Augmented | ‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡πÉ‡∏´‡πâ AI ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ |
| Generation | ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à |

---

# üí° ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÉ‡∏ô‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡∏à‡∏£‡∏¥‡∏á

> ‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏≤‡∏ñ‡∏≤‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏ß‡πà‡∏≤  
> ‚Äú‡∏ö‡∏ó‡∏ó‡∏µ‡πà 3 ‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå ‡∏û‡∏π‡∏î‡∏ñ‡∏∂‡∏á‡∏≠‡∏∞‡πÑ‡∏£‡∏ô‡∏∞?‚Äù

‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏à‡∏∞‡πÑ‡∏õ‡πÄ‡∏õ‡∏¥‡∏î‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠  
‚Üí ‡∏≠‡πà‡∏≤‡∏ô‡∏ö‡∏≤‡∏á‡∏¢‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤  
‚Üí ‡πÅ‡∏•‡πâ‡∏ß‡∏™‡∏£‡∏∏‡∏õ‡∏ï‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢

üìò ‡∏ô‡∏±‡πà‡∏ô‡πÅ‡∏´‡∏•‡∏∞! ‡∏Ñ‡∏∑‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á **RAG Framework**

---

# ‚öôÔ∏è ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö RAG

1. üìÑ **‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏¥‡πâ‡∏ô‡πÄ‡∏•‡πá‡∏Å ‡πÜ (Chunking)**  
2. üß† **‡πÉ‡∏´‡πâ AI ‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏™‡πà‡∏ß‡∏ô (Context)**  
3. üî¢ **‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (Embedding)**  
4. üîç **‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á (Retriever)**  
5. üí¨ **‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á (Generative Model)**  

---

# üß© ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1: Chunking

AI ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡πÄ‡∏•‡πà‡∏°‡πÑ‡∏î‡πâ  
‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á ‚Äú‡∏´‡∏±‡πà‡∏ô‚Äù ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏¢‡πà‡∏≠‡∏¢ ‡πÜ

```python
text = "‡∏™‡∏°‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå‡∏°‡∏µ‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏õ‡∏£‡∏∞‡∏™‡∏≤‡∏ó‡∏°‡∏≤‡∏Å‡∏°‡∏≤‡∏¢..."
chunks = [text[i:i+50] for i in range(0, len(text), 50)]
print(chunks)
````

üìò *‡∏ó‡∏≥‡πÉ‡∏´‡πâ AI ‡∏¢‡πà‡∏≠‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô*

---

# üß† ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏î‡πâ‡∏ß‡∏¢ LLM

‡πÉ‡∏ä‡πâ **ChatGPT / Llama 3 / Gemini**
‡∏ä‡πà‡∏ß‡∏¢ ‚Äú‡∏™‡∏£‡∏∏‡∏õ‚Äù ‡∏´‡∏£‡∏∑‡∏≠ ‚Äú‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‚Äù ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏™‡πà‡∏ß‡∏ô

```python
prompt = "‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢:\n\n" + chunk
```

üìò *AI ‡∏à‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô*

---

# üî¢ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 3: ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (Embedding)

AI ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à ‚Äú‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‚Äù ‡∏ú‡πà‡∏≤‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ß‡πà‡∏≤ **‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå (Vector)**

```python
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')
vec = model.encode(["‡∏™‡∏°‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå..."])
print(vec[:10])
```

üß© ‚Äú‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‚Äù ‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÅ‡∏ó‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°

---

# üíæ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 4: ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á (Retrieval)

```python
from sklearn.metrics.pairwise import cosine_similarity
score = cosine_similarity(query_vec, embeddings)
```

üìò *‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏°‡∏≤‡∏Å ‡πÅ‡∏õ‡∏•‡∏ß‡πà‡∏≤‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏°‡∏≤‡∏Å*

---

# üí¨ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 5: ‡πÉ‡∏´‡πâ AI ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö

```python
prompt = f"‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡πÇ‡∏î‡∏¢‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:\n{docs}\n‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}"
```

> AI ‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á
> ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‚Äú‡πÄ‡∏î‡∏≤‚Äù ‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•

---

# ü§ñ ‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡∏£‡∏∞‡∏ö‡∏ö RAG ‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢‡πÅ‡∏•‡πâ‡∏ß!

üß© ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î

1Ô∏è‚É£ ‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏¥‡πâ‡∏ô‡πÄ‡∏•‡πá‡∏Å ‡πÜ
2Ô∏è‚É£ ‡πÉ‡∏´‡πâ AI ‡∏™‡∏£‡∏∏‡∏õ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏ö‡∏£‡∏¥‡∏ö‡∏ó
3Ô∏è‚É£ ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå
4Ô∏è‚É£ ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
5Ô∏è‚É£ ‡πÉ‡∏´‡πâ AI ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÉ‡∏´‡∏°‡πà

---

# üéÆ Workshop Activity

### üîπ ‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô Google Colab

1. ‡πÄ‡∏õ‡∏¥‡∏î‡πÑ‡∏ü‡∏•‡πå `RAG_Workshop.ipynb`
2. ‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å‡πÇ‡∏Ñ‡πâ‡∏î‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô
3. ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏ä‡πà‡∏ô

   * ‚Äú‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå‡∏Ñ‡∏¥‡∏î‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?‚Äù
   * ‚Äú‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏£‡∏∞‡∏™‡∏≤‡∏ó‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?‚Äù
4. ‡∏î‡∏π‡∏ß‡πà‡∏≤ AI ‡∏ï‡∏≠‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£

---

# üß© Workshop Challenge

> ‡∏™‡∏£‡πâ‡∏≤‡∏á ‚ÄúAI ‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏≠‡πà‡∏≤‡∏ô‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‚Äù ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏≠‡∏á

### ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:

* üìò ‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå ‡∏ö‡∏ó 1
* üìó ‡∏™‡∏£‡∏∏‡∏õ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ä‡∏≤‡∏ï‡∏¥‡πÑ‡∏ó‡∏¢
* üìô ‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏≤‡∏£‡∏∞‡∏à‡∏≤‡∏Å‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå

---

# üß≠ ‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ

| ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô    | ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢                    |
| ---------- | --------------------------- |
| Chunking   | ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏¢‡πà‡∏≠‡∏¢  |
| Context    | ‡πÉ‡∏´‡πâ AI ‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏™‡πà‡∏ß‡∏ô |
| Embedding  | ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç       |
| Retrieval  | ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á   |
| Generation | ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á     |

---

# üí° ‡∏ñ‡∏≤‡∏°‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á‡∏î‡∏π‡∏™‡∏¥‚Ä¶

* ‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏≤‡∏ô‡∏≥ RAG ‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö ‚Äú‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÉ‡∏ô‡∏´‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‚Äù ‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á?
* AI ‡∏à‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤ ‚Äú‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‚Äù ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡πÅ‡∏Ñ‡πà ‚Äú‡∏à‡∏≥‚Äù ‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?
* ‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏≤‡∏Å‡πÉ‡∏´‡πâ‡∏°‡∏±‡∏ô‡∏â‡∏•‡∏≤‡∏î‡∏Ç‡∏∂‡πâ‡∏ô ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö‡πÑ‡∏´‡∏ô?

---

# üöÄ ‡∏†‡∏≤‡∏£‡∏Å‡∏¥‡∏à‡∏ï‡πà‡∏≠‡∏¢‡∏≠‡∏î (Extension Ideas)

* üß† ‡∏ó‡∏≥ RAG ‡∏Å‡∏±‡∏ö **‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå**
* üí¨ ‡∏ó‡∏≥ Chatbot ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö **‡∏ï‡∏¥‡∏ß‡∏™‡∏≠‡∏ö O-NET**
* üìö ‡∏ó‡∏≥‡∏™‡∏£‡∏∏‡∏õ‡∏ö‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÅ‡∏ö‡∏ö‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥

---

# üëè ‡∏™‡∏£‡∏∏‡∏õ Workshop ‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ

üéØ ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏ô‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤
AI ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÑ‡∏î‡πâ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£
**‚Äú‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ (Retrieve)‚Äù + ‚Äú‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö (Generate)‚Äù**

üìò ‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ß‡πà‡∏≤ **RAG Framework**
‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á ChatGPT, Gemini, ‡πÅ‡∏•‡∏∞ Copilot

---

# üßë‚Äçüíª ‡∏™‡∏£‡πâ‡∏≤‡∏á AI ‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏≠‡∏á!

> ‚Äú‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ ‡πÄ‡∏ß‡∏•‡∏≤‡∏≠‡πà‡∏≤‡∏ô‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠‡∏™‡∏≠‡∏ö
> ‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏≠‡∏µ‡∏Å‡∏ï‡πà‡∏≠‡πÑ‡∏õ‚Äù

üåü ‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡∏£‡πà‡∏ß‡∏° Workshop
üë®‚Äçüè´ ‡∏ä‡∏ô‡∏±‡∏ô‡∏ó‡πå‡∏Å‡∏£‡∏ì‡πå  ‡∏à‡∏±‡∏ô‡πÅ‡∏î‡∏á ‚Äì Walailak University


