{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3595677c",
   "metadata": {},
   "source": [
    "# 03 - การวิเคราะห์ข้อความเชิงสถิติ (Statistical Text Analysis)\n",
    "\n",
    "การวิเคราะห์ข้อความเชิงสถิติเป็นกระบวนการทาง Data Science ที่ใช้เทคนิคทางคณิตศาสตร์และสถิติในการสกัดความหมายและรูปแบบจากข้อมูลข้อความ (Text Data) ซึ่งโดยทั่วไปเป็นข้อมูลที่ไม่มีโครงสร้าง เช่น ข่าว โพสต์ในสื่อสังคมออนไลน์ หรือบทความวิชาการ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eeea698",
   "metadata": {},
   "source": [
    "## 1. Text Preprocessing (การเตรียมข้อมูลข้อความ)\n",
    "\n",
    "ขั้นตอนหลัก:\n",
    "- Tokenization: การตัดคำ\n",
    "- Stopword Removal: การลบคำฟุ่มเฟือย\n",
    "- Lemmatization: การแปลงคำให้อยู่ในรูปมาตรฐาน\n",
    "- การลบสัญลักษณ์ ตัวเลข หรือเครื่องหมายวรรคตอน"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21184c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "text = \"Statistical text analysis involves processing and understanding text data using math and statistics.\"\n",
    "\n",
    "tokens = word_tokenize(text.lower())\n",
    "filtered_tokens = [w for w in tokens if w not in stopwords.words('english') and w not in string.punctuation]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "print(\"Original:\", tokens)\n",
    "print(\"After Cleaning:\", lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b26b54",
   "metadata": {},
   "source": [
    "## 2. Statistical Representation (การแทนค่าข้อความเชิงสถิติ)\n",
    "\n",
    "แนวคิดหลัก:\n",
    "- Bag of Words (BoW)\n",
    "- TF-IDF (Term Frequency – Inverse Document Frequency)\n",
    "- N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198df2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"Artificial intelligence and machine learning are related fields.\",\n",
    "    \"Statistical text analysis is part of natural language processing.\"\n",
    "]\n",
    "\n",
    "bow = CountVectorizer()\n",
    "bow_matrix = bow.fit_transform(corpus)\n",
    "print(\"BoW Vocabulary:\", bow.get_feature_names_out())\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(corpus)\n",
    "print(\"TF-IDF Vocabulary:\", tfidf.get_feature_names_out())\n",
    "\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "ngram_matrix = ngram_vectorizer.fit_transform(corpus)\n",
    "print(\"Bigrams:\", ngram_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a135fa",
   "metadata": {},
   "source": [
    "## 3. Descriptive Statistics (การวิเคราะห์เชิงพรรณนา)\n",
    "\n",
    "- การนับความถี่ของคำ (Word Frequency)\n",
    "- การสร้างแผนภาพคำ (Word Cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad5a98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "words = [\"data\", \"text\", \"analysis\", \"data\", \"text\", \"machine\", \"learning\", \"data\"]\n",
    "\n",
    "freq = Counter(words)\n",
    "print(freq.most_common(5))\n",
    "\n",
    "wc = WordCloud(width=800, height=400, background_color='white').generate(\" \".join(words))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e8365e",
   "metadata": {},
   "source": [
    "## 4. Inferential and Predictive Analysis (การวิเคราะห์เชิงอ้างอิงและพยากรณ์)\n",
    "\n",
    "### 4.1 Topic Modeling (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d0789b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "documents = [\n",
    "    \"Machine learning improves predictive analytics.\",\n",
    "    \"Deep learning is part of artificial intelligence.\",\n",
    "    \"Statistical models help in natural language processing.\"\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=2, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "for idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {idx}:\")\n",
    "    print([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-5:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530cffe7",
   "metadata": {},
   "source": [
    "### 4.2 Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a33b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "texts = [\"I love this product!\", \"This is the worst experience ever.\"]\n",
    "for t in texts:\n",
    "    sentiment = TextBlob(t).sentiment.polarity\n",
    "    print(f\"'{t}' → Sentiment Score: {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce9a3cd",
   "metadata": {},
   "source": [
    "### 4.3 Clustering / Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612815c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "texts = [\n",
    "    \"Artificial intelligence and machine learning\",\n",
    "    \"Statistical analysis in data science\",\n",
    "    \"Deep learning for NLP applications\",\n",
    "    \"Predictive modeling and statistics\"\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "for i, label in enumerate(kmeans.labels_):\n",
    "    print(f\"Text: {texts[i]} → Cluster: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22f92c6",
   "metadata": {},
   "source": [
    "## 5. แนวคิดเชิงทฤษฎีที่เกี่ยวข้อง\n",
    "\n",
    "| สาขาวิชา | คำอธิบาย |\n",
    "|------------|------------|\n",
    "| Corpus Linguistics | การศึกษาภาษาโดยใช้คลังข้อมูลข้อความขนาดใหญ่เพื่อวิเคราะห์เชิงสถิติ |\n",
    "| Information Retrieval (IR) | การค้นหาและดึงข้อมูลที่เกี่ยวข้องจากข้อความจำนวนมาก |\n",
    "| Natural Language Processing (NLP) | รากฐานสำคัญของการประมวลผลภาษาเชิงคำนวณ เช่น การทำความเข้าใจและสร้างข้อความอัตโนมัติ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6acd89",
   "metadata": {},
   "source": [
    "## 6. ตัวอย่างการประยุกต์ใช้งานจริง\n",
    "\n",
    "| การประยุกต์ | คำอธิบาย |\n",
    "|---------------|------------|\n",
    "| วิเคราะห์โพสต์ในสื่อสังคมออนไลน์ | ตรวจจับอารมณ์ของผู้ใช้ต่อเหตุการณ์ทางสังคมหรือรัฐบาล |\n",
    "| วิเคราะห์ข่าวเศรษฐกิจ | สกัดหัวข้อและแนวโน้มจากบทความข่าว |\n",
    "| วิเคราะห์รีวิวสินค้า | ตรวจสอบความพึงพอใจของลูกค้า |\n",
    "| วิเคราะห์บทความวิชาการ | ใช้ TF-IDF เพื่อดึงคำสำคัญและสรุปเนื้อหา |"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
