## Transformer : Overview

### üîπ ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á Transformer

‡πÉ‡∏ô‡∏Å‡∏£‡∏≠‡∏ö‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô‡∏Ç‡∏≠‡∏á‡∏†‡∏≤‡∏û ‡πÅ‡∏™‡∏î‡∏á‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• Transformer ‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:

* **Inputs:** ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö ‡πÄ‡∏ä‡πà‡∏ô ‚ÄúHello world!‚Äù
* **The Transformer:** ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏Å‡∏•‡πÑ‡∏Å *attention* ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ñ‡∏≥‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏∂‡πà‡∏á‡∏û‡∏≤‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö (‡πÄ‡∏ä‡πà‡∏ô RNN ‡∏´‡∏£‡∏∑‡∏≠ LSTM)
* **Outputs:** ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå ‡πÄ‡∏ä‡πà‡∏ô ‚ÄúBonjour le monde!‚Äù ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏ù‡∏£‡∏±‡πà‡∏á‡πÄ‡∏®‡∏™

‡∏Å‡∏•‡πà‡∏≤‡∏ß‡πÇ‡∏î‡∏¢‡∏¢‡πà‡∏≠ ‡πÇ‡∏°‡πÄ‡∏î‡∏• Transformer ‡∏à‡∏∞‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏†‡∏≤‡∏©‡∏≤‡∏ù‡∏£‡∏±‡πà‡∏á‡πÄ‡∏®‡∏™‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô

### üîπ ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2: ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏Ç‡∏≠‡∏á Transformer

‡∏Å‡∏£‡∏≠‡∏ö‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏†‡∏≤‡∏û‡πÅ‡∏™‡∏î‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡∏ã‡∏∂‡πà‡∏á‡πÅ‡∏ö‡πà‡∏á‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≠‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏Ñ‡∏∑‡∏≠:

1. **Encoder:**

   * ‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏¥‡∏ô‡∏û‡∏∏‡∏ï (Input) ‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô **features** ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÅ‡∏ó‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥
   * ‡πÉ‡∏ä‡πâ‡∏Å‡∏•‡πÑ‡∏Å **Self-Attention** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ

2. **Decoder:**

   * ‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Encoder ‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡∏•‡∏∞‡∏Ñ‡∏≥
   * ‡πÉ‡∏ä‡πâ‡∏Å‡∏•‡πÑ‡∏Å **Masked Self-Attention** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡πÉ‡∏´‡∏°‡πà‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô


### üîπ ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 3: ‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç ‚Äì ‚ÄúAttention‚Äù

‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• Transformer ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏•‡πÑ‡∏Å **Attention** ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞ *Self-Attention* ‡∏ã‡∏∂‡πà‡∏á‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• ‚Äú‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ô‡πÉ‡∏à‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‚Äù ‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô

* ‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ ‚Äúworld‚Äù ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ ‚ÄúHello‚Äù ‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
* ‡∏™‡∏¥‡πà‡∏á‡∏ô‡∏µ‡πâ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ Transformer ‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤‡∏£‡∏∏‡πà‡∏ô‡∏Å‡πà‡∏≠‡∏ô ‡πÜ (‡πÄ‡∏ä‡πà‡∏ô RNN/LSTM) ‡∏ó‡∏±‡πâ‡∏á‡πÉ‡∏ô‡πÅ‡∏á‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå

### üîπ ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á

> Vaswani et al. (2017). *Attention Is All You Need.* [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)

##  ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ó‡∏≤‡∏á‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏°‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• Transformer (Transformer Model Architecture)

**‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ó‡∏≤‡∏á‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏°‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• Transformer (Transformer Model Architecture)** ‡∏à‡∏≤‡∏Å‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢ *Attention Is All You Need* (Vaswani et al., 2017) ‡∏ã‡∏∂‡πà‡∏á‡∏ñ‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏†‡∏≤‡∏©‡∏≤‡∏™‡∏°‡∏±‡∏¢‡πÉ‡∏´‡∏°‡πà ‡πÄ‡∏ä‡πà‡∏ô GPT, BERT, ‡πÅ‡∏•‡∏∞ T5


### üîπ 1. ‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏°

‡∏™‡∏ñ‡∏≤‡∏õ‡∏±‡∏ï‡∏¢‡∏Å‡∏£‡∏£‡∏° Transformer ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡∏™‡∏≠‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏Ñ‡∏∑‡∏≠:

* **Encoder (‡∏ã‡πâ‡∏≤‡∏¢)** ‚Äì ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤ (‡πÄ‡∏ä‡πà‡∏ô ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©)
* **Decoder (‡∏Ç‡∏ß‡∏≤)** ‚Äì ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≤‡∏≠‡∏≠‡∏Å (‡πÄ‡∏ä‡πà‡∏ô ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏†‡∏≤‡∏©‡∏≤‡πÅ‡∏õ‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏ù‡∏£‡∏±‡πà‡∏á‡πÄ‡∏®‡∏™)

‡∏ó‡∏±‡πâ‡∏á Encoder ‡πÅ‡∏•‡∏∞ Decoder ‡∏à‡∏∞‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡∏´‡∏•‡∏≤‡∏¢ ‚Äú‡∏ä‡∏±‡πâ‡∏ô (layers)‚Äù ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô‡∏ã‡πâ‡∏≥ ‡πÜ (‡πÉ‡∏ô‡∏†‡∏≤‡∏û‡πÅ‡∏ó‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå ‚ÄúNx‚Äù)

---

### üîπ 2. ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÉ‡∏ô‡∏ù‡∏±‡πà‡∏á Encoder

#### üüß ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1: Input Embedding

* ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏¥‡∏ô‡∏û‡∏∏‡∏ï (‡πÄ‡∏ä‡πà‡∏ô ‚ÄúHello world‚Äù) ‡∏ñ‡∏π‡∏Å‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ú‡πà‡∏≤‡∏ô **Embedding Layer**
* ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥‡∏à‡∏∞‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏°‡∏¥‡∏ï‡∏¥‡∏Ñ‡∏á‡∏ó‡∏µ‡πà (‡πÄ‡∏ä‡πà‡∏ô 512 ‡∏°‡∏¥‡∏ï‡∏¥)

#### üü¶ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2: Positional Encoding

* ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å Transformer ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÄ‡∏ß‡∏•‡∏≤ (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô RNN)
  ‡∏à‡∏∂‡∏á‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‚Äú‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‚Äù (*Positional Encoding*)
* Encoding ‡∏ô‡∏µ‡πâ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏Ñ‡∏≥‡πÑ‡∏´‡∏ô‡∏°‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏´‡∏•‡∏±‡∏á

‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏ö‡∏ß‡∏Å‡∏Å‡∏±‡∏ô ‚Üí ‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà Encoder block

#### üî∏ **Encoder Block (‡∏ó‡∏≥‡∏ã‡πâ‡∏≥ Nx ‡∏ä‡∏±‡πâ‡∏ô)**

‡πÅ‡∏ï‡πà‡∏•‡∏∞ block ‡∏Ç‡∏≠‡∏á Encoder ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢ 2 ‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏Å:

##### 2.1. Multi-Head Self-Attention

* ‡∏Å‡∏•‡πÑ‡∏Å‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• ‚Äú‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ô‡πÉ‡∏à‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏≠‡∏∑‡πà‡∏ô ‡πÜ ‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‚Äù
* ‚ÄúMulti-Head‚Äù ‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏≤‡∏¢‡∏ä‡∏∏‡∏î‡∏Ç‡∏≠‡∏á Attention ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ñ‡∏≥
* Self-Attention ‡∏à‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ß‡πà‡∏≤‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥‡∏Ñ‡∏ß‡∏£‡πÉ‡∏´‡πâ ‚Äú‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‚Äù ‡πÅ‡∏Å‡πà‡∏Ñ‡∏≥‡∏≠‡∏∑‡πà‡∏ô‡∏°‡∏≤‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô

##### 2.2. Add & Norm

* ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å Self-Attention ‡∏à‡∏∞‡∏°‡∏µ‡∏Å‡∏≤‡∏£ **Add** (‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏Ñ‡πà‡∏≤‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡∏î‡πâ‡∏ß‡∏¢ ‚Äúresidual connection‚Äù)
* ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡∏ó‡∏≥ **Normalization** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ

##### 2.3. Feed Forward Network

* ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏±‡πâ‡∏ô Neural Network ‡∏™‡∏≠‡∏á‡∏ä‡∏±‡πâ‡∏ô (Fully Connected Layer)
* ‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏à‡∏≤‡∏Å Attention ‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô

‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏à‡∏∞‡∏°‡∏µ **Add & Norm** ‡πÄ‡∏ä‡πà‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô

‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏à‡∏≤‡∏Å Encoder ‡∏ó‡∏∏‡∏Å‡∏ä‡∏±‡πâ‡∏ô‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏™‡πà‡∏á‡πÑ‡∏õ‡∏¢‡∏±‡∏á Decoder

 
### üîπ 3. ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÉ‡∏ô‡∏ù‡∏±‡πà‡∏á Decoder

#### üüß ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1: Output Embedding

* ‡∏ù‡∏±‡πà‡∏á Decoder ‡∏à‡∏∞‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏¢‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏°‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏µ‡πâ (‡πÄ‡∏ä‡πà‡∏ô ‡∏Ñ‡∏≥‡πÉ‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏ù‡∏£‡∏±‡πà‡∏á‡πÄ‡∏®‡∏™‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô)
* ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡∏≥‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏î‡πâ‡∏ß‡∏¢ **Embedding Layer** ‡πÄ‡∏ä‡πà‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ù‡∏±‡πà‡∏á Encoder

#### üü¶ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2: Positional Encoding

* ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏î‡πâ‡∏ß‡∏¢ **Positional Encoding** ‡πÄ‡∏ä‡πà‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô

 
#### üî∏ **Decoder Block (‡∏ó‡∏≥‡∏ã‡πâ‡∏≥ Nx ‡∏ä‡∏±‡πâ‡∏ô)**

‡πÅ‡∏ï‡πà‡∏•‡∏∞ block ‡∏Ç‡∏≠‡∏á Decoder ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢ 3 ‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏Å:

##### 3.1. Masked Multi-Head Attention

* ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô Self-Attention ‡πÅ‡∏ï‡πà‡πÄ‡∏û‡∏¥‡πà‡∏° ‚ÄúMask‚Äù ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏´‡πá‡∏ô‡∏Ñ‡∏≥‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï (‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á)
* ‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà 3 ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà 1 ‡πÅ‡∏•‡∏∞ 2

##### 3.2. Multi-Head Attention (‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Encoder)

* ‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏Å‡∏±‡∏ö Encoder ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
* ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ ‚Äú‡∏ß‡πà‡∏≤‡∏Ñ‡∏≥‡πÉ‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏Ç‡∏≤‡∏≠‡∏≠‡∏Å‡∏Ñ‡∏ß‡∏£‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ô‡πÉ‡∏à‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡πÉ‡∏î‡πÉ‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏Ç‡∏≤‡πÄ‡∏Ç‡πâ‡∏≤‚Äù
  ‡πÄ‡∏ä‡πà‡∏ô ‚ÄúBonjour‚Äù ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏Å‡∏±‡∏ö ‚ÄúHello‚Äù

##### 3.3. Feed Forward Network + Add & Norm

* ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ö‡∏ù‡∏±‡πà‡∏á Encoder

 
### üîπ 4. ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢: ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (Output Generation)

‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å Decoder ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠‡∏ú‡πà‡∏≤‡∏ô‡∏™‡∏≠‡∏á‡∏ä‡∏±‡πâ‡∏ô:

1. **Linear Layer** ‚Äì ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡πÉ‡∏ô‡∏û‡∏à‡∏ô‡∏≤‡∏ô‡∏∏‡∏Å‡∏£‡∏° (vocabulary size)
2. **Softmax Layer** ‚Äì ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô ‚Äú‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‚Äù ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥

‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡∏à‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå ‡πÄ‡∏ä‡πà‡∏ô

> ‚ÄúHello world‚Äù ‚Üí ‚ÄúBonjour le monde‚Äù

 
### üîπ 5. ‡∏™‡∏£‡∏∏‡∏õ Flow ‡πÇ‡∏î‡∏¢‡∏¢‡πà‡∏≠

| ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô | ‡∏™‡πà‡∏ß‡∏ô‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö                            | ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà                                      |
| ------- | ------------------------------------- | -------------------------------------------- |
| 1       | Input Embedding + Positional Encoding | ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡∏≥‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á                  |
| 2       | Encoder Stack (Nx)                    | ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Ç‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏≠‡∏¥‡∏ô‡∏û‡∏∏‡∏ï                 |
| 3       | Decoder Stack (Nx)                    | ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Encoder ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå |
| 4       | Linear + Softmax                      | ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥‡∏à‡∏£‡∏¥‡∏á ‡πÜ                     |

 
### üîπ ‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç

Transformer ‡πÉ‡∏ä‡πâ‡∏Å‡∏•‡πÑ‡∏Å **Self-Attention** ‡πÅ‡∏•‡∏∞ **Parallel Processing** ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö ‡∏ó‡∏≥‡πÉ‡∏´‡πâ:

* ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πà‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô
* ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏î‡πâ‡∏£‡∏ß‡∏î‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ RNN
* ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Ç‡∏¢‡∏≤‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà ‡πÄ‡∏ä‡πà‡∏ô GPT ‡πÅ‡∏•‡∏∞ BERT

