# LLM-Workshop

## **1. พื้นฐานของ Large Language Models (LLMs)**

Large Language Models (LLMs) คือ **โมเดลปัญญาประดิษฐ์เชิงสถิติ** (Statistical AI Models) ที่ใช้เทคนิคการเรียนรู้เชิงลึก (*Deep Learning*) เพื่อสร้างแบบจำลองภาษา (Language Modeling) โดยทำการ **เรียนรู้โครงสร้าง รูปแบบ และความสัมพันธ์ของภาษา** จากข้อมูลข้อความจำนวนมหาศาล โมเดลเหล่านี้สามารถ **เข้าใจภาษา (Natural Language Understanding: NLU)** และ **สร้างภาษา (Natural Language Generation: NLG)** ได้อย่างใกล้เคียงกับมนุษย์

## **1.1 แนวคิดพื้นฐานของการสร้างโมเดลภาษา**

LLMs ทำงานบนหลักการสำคัญของ **การประมาณความน่าจะเป็นของลำดับคำ** (*Sequence Modeling*) กล่าวคือ โมเดลจะพยายามเรียนรู้ความสัมพันธ์ของคำในประโยค เพื่อทำนายคำถัดไปที่มีความน่าจะเป็นสูงสุด

### **สมการพื้นฐาน**

โมเดลคำนวณความน่าจะเป็นของข้อความ $X$ ที่ประกอบด้วยลำดับของคำ $(w_1, w_2, ..., w_n)$ ตามสูตร:

$$
P(X) = P(w_1, w_2, ..., w_n) = \prod_{t=1}^{n} P(w_t | w_1, w_2, ..., w_{t-1})
$$

ซึ่งหมายความว่า **ความน่าจะเป็นของคำในตำแหน่งที่ $t$** ขึ้นกับบริบทก่อนหน้า $(w_1 ... w_{t-1})$

**ตัวอย่าง:**

> ประโยค: "ฉันกำลังเขียน \_\_\_"
> โมเดลจะพิจารณาบริบทก่อนหน้า ("ฉันกำลังเขียน") เพื่อทำนายคำที่มีความน่าจะเป็นสูงสุด เช่น:

* "โปรแกรม"
* "บทความ"
* "จดหมาย"

## **1.2 การแปลงข้อความเป็นตัวเลข (Tokenization & Embeddings)**

เนื่องจากโมเดล Deep Learning ไม่สามารถประมวลผลตัวอักษรโดยตรง จึงต้องมีขั้นตอน **แปลงข้อความ → ตัวเลข** ผ่านสองกระบวนการสำคัญ:

### **(a) Tokenization**

* **Token** คือหน่วยย่อยของข้อความ เช่น ตัวอักษร คำ หรือพยางค์
* โมเดล LLM เช่น GPT ใช้วิธี **Byte-Pair Encoding (BPE)** หรือ **SentencePiece** เพื่อแบ่งข้อความออกเป็น tokens
* ตัวอย่างการ Tokenize:

  ```
  "สวัสดีครับ" → ["สวั", "สดี", "ครับ"]
  ```

### **(b) Embeddings**

* Token แต่ละตัวจะถูกแปลงเป็น **เวกเตอร์เชิงตัวเลข** (Vector Representation) ในมิติสูง ๆ เช่น 1,024 มิติ หรือ 4,096 มิติ
* เวกเตอร์นี้ทำให้โมเดลสามารถ "เข้าใจ" ความหมายและความสัมพันธ์ระหว่างคำ เช่น:

  ```
  Embedding("แมว") ≈ [0.12, -0.56, 0.34, ...]
  Embedding("สุนัข") ≈ [0.15, -0.53, 0.31, ...]
  ```
* เมื่อวัดระยะทางของเวกเตอร์ด้วย **Cosine Similarity** จะพบว่า "แมว" และ "สุนัข" ใกล้กันมากกว่า "แมว" กับ "รถยนต์" → โมเดลจึงเข้าใจความเชิงความหมายของคำ
  
## **1.3 สถาปัตยกรรม Transformer: หัวใจของ LLM**

โมเดล GPT-OSS และ LLM สมัยใหม่เกือบทั้งหมดใช้สถาปัตยกรรม **Transformer** ซึ่งถูกเสนอครั้งแรกในงานวิจัย *“Attention Is All You Need”* (Vaswani et al., 2017)

### **องค์ประกอบสำคัญ**

1. **Self-Attention Mechanism**

   * ช่วยให้โมเดลคำนวณว่า "แต่ละคำควรสนใจคำอื่น ๆ ในบริบทแค่ไหน"
   * ตัวอย่าง: ในประโยค

     > "แมวกินปลาเพราะมันหิว"
     > คำว่า "มัน" จะ **Attention** ไปยัง "แมว" มากกว่าคำอื่น เพราะเกี่ยวข้องกันในบริบท

2. **Multi-Head Attention**

   * แทนที่จะสนใจบริบทเดียว โมเดลสนใจหลาย ๆ มุมมองพร้อมกัน → ทำให้เข้าใจความหมายซับซ้อน

3. **Feedforward Neural Network**

   * หลังจาก Attention โมเดลจะผ่านเครือข่ายประสาทลึก เพื่อเรียนรู้ความสัมพันธ์ที่ซับซ้อนระหว่าง tokens

4. **Positional Encoding**

   * Transformer ไม่มีลำดับในตัว จึงต้องเพิ่มข้อมูลตำแหน่งของคำ เพื่อให้โมเดลรู้ว่าคำใดมาก่อน-หลัง


## **1.4 การเรียนรู้แบบ Pre-training และ Fine-tuning**

### **(a) Pre-training**

* โมเดล LLM เช่น GPT-OSS ถูกฝึกบน **Corpus ขนาดใหญ่** ที่มีจำนวน **หลายล้านล้าน tokens**
* เป้าหมาย: ให้โมเดลเข้าใจ **ไวยากรณ์** (**Syntax**) และ **ความหมาย** (**Semantics**) ของภาษาหลากหลาย

### **(b) Fine-tuning**

* หลังจาก Pre-training โมเดลสามารถ **ปรับจูนเฉพาะงาน** (Task-specific) ได้ เช่น:

  * สร้าง Chatbot
  * วิเคราะห์เอกสารเฉพาะ
  * ทำงาน Reasoning เช่น เขียนโค้ดหรือวิเคราะห์ข้อมูล


## **1.5 ความสัมพันธ์ระหว่างขนาดโมเดล ข้อมูล และประสิทธิภาพ**

งานวิจัยของ OpenAI และ DeepMind แสดงให้เห็นว่า **กฎการสเกล (Scaling Laws)** มีผลโดยตรงต่อประสิทธิภาพของโมเดล:

| ปัจจัย                       | มีผลต่อ                           | ตัวอย่างใน GPT-OSS                  |
| ---------------------------- | --------------------------------- | ----------------------------------- |
| **พารามิเตอร์ (Parameters)** | ความสามารถในการ “จำ” และ “เข้าใจ” | GPT-OSS-20B → 20 พันล้านพารามิเตอร์ |
| **ขนาดข้อมูล**               | ความหลากหลายของคำตอบ              | เทรนจาก **หลายล้านล้าน tokens**     |
| **คอมพิวต์**                 | เวลาในการฝึกและประสิทธิภาพ        | ใช้ GPU/TPU ระดับ Data Center       |

ยิ่งโมเดลใหญ่ → ยิ่งสามารถเข้าใจภาษาซับซ้อนได้ดี แต่ก็ใช้ทรัพยากรสูงขึ้น


## **1.6 ความสามารถหลักของ LLM**

1. **Natural Language Understanding (NLU)** → วิเคราะห์ความหมายข้อความ
2. **Natural Language Generation (NLG)** → สร้างข้อความใหม่ที่สมเหตุสมผล
3. **Reasoning** → แก้ปัญหาเชิงตรรกะและวิเคราะห์ข้อมูล
4. **Zero-shot / Few-shot Learning** → โมเดลสามารถทำงานที่ไม่เคยเห็นตัวอย่างได้ดี เพียงอาศัย prompt ที่ดี


## **สรุปสำหรับผู้เริ่มต้น**

* LLM คือโมเดลที่ **เข้าใจภาษา** และ **สร้างข้อความได้** โดยใช้ **สถาปัตยกรรม Transformer**
* ข้อความถูกแปลงเป็น **token → embedding → คำนวณ self-attention → สร้างผลลัพธ์**
* ยิ่งโมเดลใหญ่และข้อมูลมาก → ความสามารถยิ่งสูง
* GPT-OSS-20B และ GPT-OSS-120B ใช้หลักการนี้ แต่ปรับให้เป็น **โอเพนซอร์ส** เพื่อความยืดหยุ่นและปลอดภัย


---

# **2. สถาปัตยกรรม GPT-OSS (OpenAI Open Source Models)**

**GPT-OSS** เป็นโมเดล **โอเพนซอร์ส** (Open Source) ที่พัฒนาโดย OpenAI โดยออกแบบให้ผู้ใช้สามารถ:

* **ดาวน์โหลดและติดตั้ง** บนเซิร์ฟเวอร์ของตนเอง
* **ปรับแต่ง (Fine-tune)** โมเดลให้เหมาะกับงานเฉพาะ
* **Deploy** ภายในองค์กร โดยไม่ต้องพึ่งพาเซิร์ฟเวอร์ของ OpenAI
* **ควบคุมข้อมูลได้เต็มที่** → เหมาะกับงานที่ต้องการ **Data Privacy** สูง เช่น งานวิจัย, ธุรกิจ, หน่วยงานรัฐ

หัวใจของ GPT-OSS คือ **สถาปัตยกรรม Transformer** ซึ่งถูกใช้ใน GPT-OSS-20B และ GPT-OSS-120B ที่ถูกปล่อยล่าสุด

# **2.1 สถาปัตยกรรม Transformer**

Transformer ถูกเสนอครั้งแรกในงานวิจัย **“Attention Is All You Need” (Vaswani et al., 2017)** และกลายเป็นมาตรฐานของ **LLM (Large Language Models)** ทุกตัวในปัจจุบัน ไม่ว่าจะเป็น GPT, LLaMA, Claude หรือ Gemini

GPT-OSS ใช้ **Decoder-only Transformer** ซึ่งหมายความว่าโมเดลถูกออกแบบให้สร้างข้อความออกมาแบบ **Autoregressive Generation** คือ ทำนาย **token ถัดไป** จากบริบทของ token ก่อนหน้า

## **องค์ประกอบหลักของ Transformer**

### **1. Encoder / Decoder Architecture**

* สถาปัตยกรรม Transformer ดั้งเดิมมี 2 ส่วน:

  * **Encoder** → ประมวลผลและเข้ารหัสข้อมูลต้นฉบับ เช่น ใช้ในงานแปลภาษา
  * **Decoder** → ใช้สร้างข้อความออกมาโดยอ้างอิงข้อมูลจาก Encoder
* **GPT-OSS** และ **GPT-series** ใช้ **Decoder-only Transformer**
  → ไม่มี Encoder เพราะ GPT ถูกออกแบบมาเพื่อสร้างข้อความใหม่ ไม่ได้โฟกัสที่การจับคู่ Input-Output แบบ Machine Translation

**การทำงานใน GPT-OSS:**

1. รับ Prompt เป็น **token embeddings**
2. ประมวลผลด้วย **Decoder blocks** หลายชั้น
3. สร้าง **token ถัดไป** ตามบริบทที่ได้รับ

### **2. Self-Attention Mechanism** *(หัวใจของ GPT-OSS)*

**Self-Attention** ช่วยให้โมเดล “เลือกสนใจ” คำสำคัญในบริบทมากกว่าคำอื่น ๆ

#### **แนวคิดหลัก**

สมมติเรามีประโยค:

> “แมวกินปลาเพราะมันหิว”

* คำว่า “มัน” หมายถึง “แมว”
* โมเดลต้องเรียนรู้ว่า **"มัน"** → **"แมว"** ไม่ใช่ **"ปลา"**
* Self-Attention คำนวณ **น้ำหนักความสำคัญ** ของแต่ละ token กับ token อื่น ๆ
* ผลลัพธ์คือโมเดลเข้าใจความสัมพันธ์เชิงบริบท (Contextual Relationship)

#### **การคำนวณ Self-Attention**

Self-Attention ใช้ **3 เมทริกซ์หลัก**:

* **Query (Q)** → สิ่งที่เรากำลังโฟกัส
* **Key (K)** → สิ่งที่ใช้ค้นหาความสัมพันธ์
* **Value (V)** → ข้อมูลที่ต้องการดึงมา

สูตรคำนวณ:

$$
Attention(Q, K, V) = \text{Softmax} \left( \frac{QK^T}{\sqrt{d_k}} \right)V
$$

* $QK^T$ → วัดความใกล้กันระหว่าง token
* $\sqrt{d_k}$ → ลดการสเกลค่าขนาดใหญ่
* Softmax → แปลงคะแนนให้เป็น “ค่าน้ำหนัก” ที่รวมกัน = 1

**ผลลัพธ์:** แต่ละคำจะมีเวกเตอร์ใหม่ที่บอกว่า “ควรสนใจคำใดบ้างในประโยค”

### **3. Multi-Head Attention**

* Self-Attention เพียงครั้งเดียวอาจไม่พอ เพราะบริบทภาษามีหลายมิติ
* Transformer จึงใช้ **หลายหัว (heads)** เพื่อให้แต่ละหัวโฟกัสในแง่มุมต่างกัน:

  * หัวหนึ่งอาจดู **ไวยากรณ์ (Syntax)**
  * อีกหัวอาจดู **ความหมาย (Semantics)**
  * อีกหัวอาจดู **ระยะเวลาของคำ (Temporal Context)**

เมื่อรวมผลลัพธ์จากหลายหัวเข้าด้วยกัน โมเดลจึงเข้าใจบริบทได้ลึกซึ้ง


### **4. Feedforward Layers**

หลังจากผ่าน Attention แต่ละ token จะถูกส่งเข้า **Feedforward Neural Network (FFN)** ซึ่งมีหลายชั้นของ **Linear Transformation** และ **Activation Functions** เช่น **ReLU** หรือ **GELU**

**หน้าที่หลักของ FFN**

* จับความสัมพันธ์ที่ซับซ้อนระหว่างเวกเตอร์
* แปลง Feature Space ให้เหมาะกับการสร้างข้อความถัดไป


### **5. Positional Encoding**

ปัญหาของ Transformer คือมัน **ไม่ได้มีโครงสร้างลำดับโดยกำเนิด** ต่างจาก RNN/LSTM ที่มีการไหลแบบ sequential จึงต้องมีการ **เพิ่มข้อมูลตำแหน่ง** ของ token

**หลักการ:**

* โมเดลจะสร้างเวกเตอร์ **Positional Encoding** ที่บอกตำแหน่งของแต่ละ token ในประโยค
* เวกเตอร์นี้ถูกบวกเข้ากับ Embedding ของ token ก่อนเข้าสู่ Attention Layer
* ทำให้โมเดลรู้ว่า:

  * คำที่ 1 → “ฉัน”
  * คำที่ 2 → “กำลัง”
  * คำที่ 3 → “เรียน”
    
## **ประโยชน์ของ Transformer ใน GPT-OSS**

| คุณสมบัติ                | ผลลัพธ์ / ความสำคัญ                                           |
| ------------------------ | ------------------------------------------------------------- |
| **รองรับข้อความยาว**     | ใช้ Self-Attention วิเคราะห์ข้อความได้หลายพัน Token           |
| **Parallel Computation** | ประมวลผลเร็วกว่า RNN/LSTM เนื่องจากทำงานแบบขนาน               |
| **เข้าใจบริบทลึกซึ้ง**   | ใช้ Multi-Head Attention เข้าใจความหมายเชิงซับซ้อน            |
| **ยืดหยุ่น**             | ปรับใช้กับงานหลายประเภท เช่น Chatbot, QA, Summarization, Code |

## **สรุปการทำงาน GPT-OSS ในหนึ่งบรรทัด**

> **GPT-OSS = Decoder-only Transformer**
> ใช้ **Self-Attention + Multi-Head Attention + Feedforward Layers + Positional Encoding** → เพื่อสร้างข้อความต่อเนื่องที่เข้าใจบริบทได้ลึก

