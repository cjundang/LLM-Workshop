## üß† 03 - ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏¥‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥ (Statistical Text Analysis)

### ‡∏ö‡∏ó‡∏ô‡∏≥‡πÄ‡∏ä‡∏¥‡∏á‡∏ó‡∏§‡∏©‡∏é‡∏µ

‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏¥‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≤‡∏á **Data Science ‡πÅ‡∏•‡∏∞ Computational Linguistics** ‡∏ó‡∏µ‡πà‡∏ô‡∏≥ **‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≤‡∏á‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡πÅ‡∏•‡∏∞‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥** ‡∏°‡∏≤‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö **‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (Text Data)** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢ ‡πÅ‡∏¢‡∏Å‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á ‡πÅ‡∏•‡∏∞‡∏´‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÇ‡∏î‡∏¢‡∏°‡∏µ‡∏à‡∏∏‡∏î‡∏°‡∏∏‡πà‡∏á‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÄ‡∏ä‡∏¥‡∏á‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ô‡∏≥‡πÑ‡∏õ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÑ‡∏î‡πâ ‡πÄ‡∏ä‡πà‡∏ô ‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó (classification), ‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ (topic modeling), ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå (sentiment analysis) ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô

## 1. Text Preprocessing (‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°)

‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡∏¥‡∏ö‡∏°‡∏±‡∏Å‡∏°‡∏µ **Noise** ‡πÄ‡∏ä‡πà‡∏ô ‡∏Ñ‡∏≥‡∏ü‡∏∏‡πà‡∏°‡πÄ‡∏ü‡∏∑‡∏≠‡∏¢ ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ß‡∏£‡∏£‡∏Ñ‡∏ï‡∏≠‡∏ô ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢ ‡∏ï‡πâ‡∏≠‡∏á‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡πà‡∏≠‡∏ô‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏Å

üìò ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô (Original Sentences)

1. *"Data scientists are analyzing large amounts of data in 2024!"*
2. *"The AI models were performing exceptionally well during the experiment."*
3. *"Machine learning helps computers understand human language."*

 

**üîπ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1: Tokenization ‚Äì ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥**

**‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≥ (Tokenization)** ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏¢‡πà‡∏≠‡∏¢ (tokens) ‡πÄ‡∏ä‡πà‡∏ô ‡∏Ñ‡∏≥‡∏´‡∏£‡∏∑‡∏≠‡∏ß‡∏•‡∏µ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÅ‡∏•‡∏∞‡∏ô‡∏≥‡πÑ‡∏õ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÑ‡∏î‡πâ

| ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á | ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏´‡∏•‡∏±‡∏á Tokenization                                                                                     |
| -------- | ------------------------------------------------------------------------------------------------------------ |
| 1        | `['Data', 'scientists', 'are', 'analyzing', 'large', 'amounts', 'of', 'data', 'in', '2024', '!']`            |
| 2        | `['The', 'AI', 'models', 'were', 'performing', 'exceptionally', 'well', 'during', 'the', 'experiment', '.']` |
| 3        | `['Machine', 'learning', 'helps', 'computers', 'understand', 'human', 'language', '.']`                      |


**‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2: Stopword Removal ‚Äì ‡∏Å‡∏≤‡∏£‡∏•‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ô‡∏±‡∏¢‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç**

**Stopword Removal** ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏•‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ ‡πÄ‡∏ä‡πà‡∏ô ‚Äúthe‚Äù, ‚Äúare‚Äù, ‚Äúof‚Äù, ‚Äúin‚Äù ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå

| ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á | ‡∏´‡∏•‡∏±‡∏á‡∏•‡∏ö Stopwords                                                                   |
| -------- | ---------------------------------------------------------------------------------- |
| 1        | `['Data', 'scientists', 'analyzing', 'large', 'amounts', 'data', '2024']`          |
| 2        | `['AI', 'models', 'performing', 'exceptionally', 'well', 'experiment']`            |
| 3        | `['Machine', 'learning', 'helps', 'computers', 'understand', 'human', 'language']` |


**‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 3: Stemming / Lemmatization ‚Äì ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô**

‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô (root form) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏ß‡∏°‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏£‡∏π‡∏õ‡πÑ‡∏ß‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡πÅ‡∏ï‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô ‡πÄ‡∏ä‡πà‡∏ô ‚Äúanalyzing‚Äù ‚Üí ‚Äúanalyze‚Äù, ‚Äúmodels‚Äù ‚Üí ‚Äúmodel‚Äù

| ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á | ‡∏´‡∏•‡∏±‡∏á Lemmatization                                                            |
| -------- | ----------------------------------------------------------------------------- |
| 1        | `['data', 'scientist', 'analyze', 'large', 'amount', 'data', '2024']`         |
| 2        | `['AI', 'model', 'perform', 'exceptionally', 'well', 'experiment']`           |
| 3        | `['machine', 'learn', 'help', 'computer', 'understand', 'human', 'language']` |

**‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 4: Symbol & Number Removal ‚Äì ‡∏Å‡∏≤‡∏£‡∏•‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á**

‡∏•‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ß‡∏£‡∏£‡∏Ñ‡∏ï‡∏≠‡∏ô ‡πÅ‡∏•‡∏∞‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏≤‡∏á‡∏†‡∏≤‡∏©‡∏≤

| ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á | ‡∏´‡∏•‡∏±‡∏á‡∏•‡∏ö‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç                                  |
| -------- | --------------------------------------------------------- |
| 1        | `"data scientist analyze large amount data"`              |
| 2        | `"AI model perform exceptionally well experiment"`        |
| 3        | `"machine learn help computer understand human language"` |

---

**üîé ‡∏™‡∏£‡∏∏‡∏õ‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á 4 ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô**

| ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô                      | ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏´‡∏•‡∏±‡∏Å                     | ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå                   |
| ---------------------------- | ------------------------------------ | ------------------------- |
| **Tokenization**             | ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏¢‡πà‡∏≠‡∏¢              | ‡πÑ‡∏î‡πâ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥        |
| **Stopword Removal**         | ‡∏•‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ó‡∏≤‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢ | ‡∏•‡∏î Noise ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•         |
| **Stemming / Lemmatization** | ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô              | ‡∏£‡∏ß‡∏°‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏£‡∏≤‡∏Å‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô     |
| **Symbol & Number Removal**  | ‡∏•‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÅ‡∏•‡∏∞‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô     | ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÄ‡∏ä‡∏¥‡∏á‡∏†‡∏≤‡∏©‡∏≤ |





### ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î Python

```python
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer, WordNetLemmatizer
import string

# ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

text = "Statistical text analysis involves processing and understanding text data using math and statistics."

# Tokenization
tokens = word_tokenize(text.lower())

# Stopword Removal
filtered_tokens = [w for w in tokens if w not in stopwords.words('english') and w not in string.punctuation]

# Lemmatization
lemmatizer = WordNetLemmatizer()
lemmatized = [lemmatizer.lemmatize(word) for word in filtered_tokens]

print("Original:", tokens)
print("After Cleaning:", lemmatized)
```

## 2. Statistical Representation (‡∏Å‡∏≤‡∏£‡πÅ‡∏ó‡∏ô‡∏Ñ‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏¥‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥)

‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÅ‡∏•‡πâ‡∏ß ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÅ‡∏ó‡∏ô‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ó‡∏≤‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÑ‡∏î‡πâ

**üìò ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á (‡πÉ‡∏ä‡πâ‡∏ä‡∏∏‡∏î‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤)**

1. *"Data scientists are analyzing large amounts of data in 2024!"*
2. *"The AI models were performing exceptionally well during the experiment."*
3. *"Machine learning helps computers understand human language."*

‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ú‡πà‡∏≤‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô **Preprocessing** ‡πÅ‡∏•‡πâ‡∏ß (‡πÄ‡∏ä‡πà‡∏ô Tokenization, Stopword Removal, Lemmatization, Symbol Removal)
‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ üëá

| Original Sentence | Cleaned Text                                            |
| ----------------- | ------------------------------------------------------- |
| 1                 | `data scientist analyze large amount data`              |
| 2                 | `AI model perform exceptionally well experiment`        |
| 3                 | `machine learn help computer understand human language` |


**üîπ 1. Bag of Words (BoW)**

**üìñ ‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î**

**Bag of Words (‡∏ñ‡∏∏‡∏á‡∏Ñ‡∏≥)** ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏ó‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£ **‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏≥‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£**
‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏™‡∏ô‡πÉ‡∏à‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥ ‡πÄ‡∏ä‡πà‡∏ô ‚Äúdata scientist analyze large amount data‚Äù
‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÅ‡∏ó‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç ‡πÄ‡∏ä‡πà‡∏ô `[data:2, scientist:1, analyze:1, ...]`

**üß† ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏ó‡∏ô‡∏Ñ‡πà‡∏≤**

| Vocabulary (‡∏£‡∏ß‡∏°‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏≥) | data | scientist | analyze | large | amount | AI | model | perform | experiment | machine | learn | language | ... |
| --------------------- | ---- | --------- | ------- | ----- | ------ | -- | ----- | ------- | ---------- | ------- | ----- | -------- | --- |
| **Sentence 1**        | 2    | 1         | 1       | 1     | 1      | 0  | 0     | 0       | 0          | 0       | 0     | 0        | ... |
| **Sentence 2**        | 0    | 0         | 0       | 0     | 0      | 1  | 1     | 1       | 1          | 0       | 0     | 0        | ... |
| **Sentence 3**        | 0    | 0         | 0       | 0     | 0      | 0  | 0     | 0       | 0          | 1       | 1     | 1        | ... |

üìä ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏à‡∏∂‡∏á‡∏ñ‡∏π‡∏Å‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô ‚Äú‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Ñ‡∏≥‚Äù
‡πÄ‡∏ä‡πà‡∏ô

* Sentence 1 ‚Üí `[2, 1, 1, 1, 1, 0, 0, ...]`
* Sentence 2 ‚Üí `[0, 0, 0, 0, 0, 1, 1, 1, ...]`

üîç **‡∏Ç‡πâ‡∏≠‡∏î‡∏µ:** ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢ ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢
‚ö†Ô∏è **‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î:** ‡πÑ‡∏°‡πà‡∏™‡∏ô‡πÉ‡∏à‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏≥ ‡πÄ‡∏ä‡πà‡∏ô ‚ÄúAI model‚Äù ‡∏Å‡∏±‡∏ö ‚Äúmodel AI‚Äù ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô

**üîπ 2. TF‚ÄìIDF (Term Frequency ‚Äì Inverse Document Frequency)**

**üìñ ‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î**

**TF‚ÄìIDF** ‡πÉ‡∏ä‡πâ‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡∏ß‡πà‡∏≤ ‚Äú‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡∏ö‡πà‡∏≠‡∏¢‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏´‡∏ô‡∏∂‡πà‡∏á ‡πÅ‡∏ï‡πà‡∏≠‡∏¢‡πà‡∏π‡πÑ‡∏°‡πà‡∏ö‡πà‡∏≠‡∏¢‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡∏∑‡πà‡∏ô ‡πÜ ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‚Äù
‡πÄ‡∏ä‡πà‡∏ô ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ *‚Äúdata‚Äù* ‡∏≠‡∏≤‡∏à‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡∏ö‡πà‡∏≠‡∏¢‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ (‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç)
‡πÅ‡∏ï‡πà‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ *‚Äúexperiment‚Äù* ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÉ‡∏ô Sentence 2 ‡∏à‡∏∞‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤

**üßÆ ‡∏™‡∏π‡∏ï‡∏£‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô**

[
\text{TF-IDF}(t, d) = TF(t, d) \times IDF(t)
]

* **TF(t, d)** = ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥ *t* ‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ *d*
* **IDF(t)** = log(‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î √∑ ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏≥ *t*)

**üß† ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á (‡πÄ‡∏ä‡∏¥‡∏á‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î)**

| ‡∏Ñ‡∏≥ (Term)      | Sentence 1 | Sentence 2 | Sentence 3 | ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏£‡∏ß‡∏° | IDF | TF-IDF ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°         |
| -------------- | ---------- | ---------- | ---------- | ---------- | --- | ---------------------- |
| **data**       | ‡∏™‡∏π‡∏á        | 0          | 0          | ‡∏™‡∏π‡∏á        | ‡∏ï‡πà‡∏≥ | ‡∏Ñ‡πà‡∏≤‡∏ï‡πà‡∏≥ (‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏û‡∏ö‡∏ó‡∏∏‡∏Å‡∏ó‡∏µ‡πà) |
| **experiment** | 0          | ‡∏™‡∏π‡∏á        | 0          | ‡∏ï‡πà‡∏≥        | ‡∏™‡∏π‡∏á | ‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á                 |
| **machine**    | 0          | 0          | ‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á    | ‡∏ï‡πà‡∏≥        | ‡∏™‡∏π‡∏á | ‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á                 |
| **perform**    | 0          | ‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á    | 0          | ‡∏ï‡πà‡∏≥        | ‡∏™‡∏π‡∏á | ‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á                 |

‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô TF‚ÄìIDF ‡∏à‡∏∞ ‚Äú‡∏•‡∏î‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏Ñ‡∏≥‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‚Äù ‡πÄ‡∏ä‡πà‡∏ô ‚Äúdata‚Äù
‡πÅ‡∏•‡∏∞ ‚Äú‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏Ñ‡∏≥‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‚Äù ‡πÄ‡∏ä‡πà‡∏ô ‚Äúexperiment‚Äù ‡∏´‡∏£‡∏∑‡∏≠ ‚Äúmachine‚Äù
‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à ‚Äú‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‚Äù (keywords) ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô


**üîπ 3. N-grams**

**üìñ ‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î**

**N-grams** ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏≥‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏π‡πà (‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤)
‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏ö ‚Äú‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏≥‚Äù ‡πÅ‡∏•‡∏∞ ‚Äú‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‚Äù ‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°

* **Unigram (N=1):** ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏Ñ‡∏≥‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß ‡πÄ‡∏ä‡πà‡∏ô `['machine', 'learning']`
* **Bigram (N=2):** ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏Ñ‡∏≥‡∏Ñ‡∏π‡πà ‡πÄ‡∏ä‡πà‡∏ô `['machine learning']`
* **Trigram (N=3):** ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏™‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á ‡πÄ‡∏ä‡πà‡∏ô `['machine learning helps']`

**üß† ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Bigram**

| ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ | Bigram ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ                                                                                                   |
| ------ | --------------------------------------------------------------------------------------------------------------- |
| 1      | `['data scientist', 'scientist analyze', 'analyze large', 'large amount', 'amount data']`                       |
| 2      | `['AI model', 'model perform', 'perform exceptionally', 'exceptionally well', 'well experiment']`               |
| 3      | `['machine learn help', 'learn help computer', 'help computer understand', 'computer understand human', 'understand human language']` |

üîç ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ **N-grams** ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥ ‡πÄ‡∏ä‡πà‡∏ô

* ‚Äúmachine learning‚Äù ‚Üí ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏î‡πâ‡∏≤‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£
* ‚Äúdeep learning‚Äù ‚Üí ‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å ‚Äúlearning deep‚Äù

**üß© ‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≤‡∏°‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ**

| ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ                 | ‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£                        | ‡∏Ç‡πâ‡∏≠‡∏î‡∏µ                            | ‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î                   | ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á                       |
| ---------------------- | ------------------------------ | -------------------------------- | -------------------------- | ------------------------------ |
| **Bag of Words (BoW)** | ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è             | ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢ ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô | ‡πÑ‡∏°‡πà‡∏™‡∏ô‡πÉ‡∏à‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡πÅ‡∏•‡∏∞‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏≥     | ‚Äúdata:2, scientist:1‚Äù          |
| **TF‚ÄìIDF**             | ‡πÉ‡∏´‡πâ‡∏Ñ‡πà‡∏≤‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥ | ‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡πÑ‡∏î‡πâ‡∏î‡∏µ    | ‡πÑ‡∏°‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥ | ‚Äúexperiment: ‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤ data‚Äù     |
| **N-grams**            | ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏Ñ‡∏≥‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á (‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏≥)   | ‡∏à‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡πÅ‡∏•‡∏∞‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡πÑ‡∏î‡πâ  | ‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ï‡∏≤‡∏° N       | ‚Äúmachine learning‚Äù, ‚ÄúAI model‚Äù |



### ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î Python

```python
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

corpus = [
    "Artificial intelligence and machine learning are related fields.",
    "Statistical text analysis is part of natural language processing."
]

# Bag of Words
bow = CountVectorizer()
bow_matrix = bow.fit_transform(corpus)
print("BoW Vocabulary:", bow.get_feature_names_out())

# TF-IDF
tfidf = TfidfVectorizer()
tfidf_matrix = tfidf.fit_transform(corpus)
print("TF-IDF Vocabulary:", tfidf.get_feature_names_out())

# N-grams Example
ngram_vectorizer = CountVectorizer(ngram_range=(2, 2))
ngram_matrix = ngram_vectorizer.fit_transform(corpus)
print("Bigrams:", ngram_vectorizer.get_feature_names_out())
```

## 3. Descriptive Statistics (‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏û‡∏£‡∏£‡∏ì‡∏ô‡∏≤)

‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÄ‡∏ä‡πà‡∏ô ‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á **Word Cloud**

### ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå

* ‡∏Å‡∏≤‡∏£‡∏ô‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥ (Word Frequency)
* ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏≥‡∏¢‡∏≠‡∏î‡∏ô‡∏¥‡∏¢‡∏° (Word Cloud)

### ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î Python

```python
from collections import Counter
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Example Tokens
words = ["data", "text", "analysis", "data", "text", "machine", "learning", "data"]

# Frequency Distribution
freq = Counter(words)
print(freq.most_common(5))

# Word Cloud
wc = WordCloud(width=800, height=400, background_color='white').generate(" ".join(words))
plt.imshow(wc, interpolation='bilinear')
plt.axis("off")
plt.show()
```

## 4. Inferential and Predictive Analysis (‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÅ‡∏•‡∏∞‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå)

‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏ó‡∏≤‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÅ‡∏•‡∏∞ Machine Learning ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ö‡∏ö‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°

### 4.1 Topic Modeling (LDA)

‡∏ä‡πà‡∏ß‡∏¢‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÇ‡∏î‡∏¢‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥

```python
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer

documents = [
    "Machine learning improves predictive analytics.",
    "Deep learning is part of artificial intelligence.",
    "Statistical models help in natural language processing."
]

vectorizer = CountVectorizer()
X = vectorizer.fit_transform(documents)

lda = LatentDirichletAllocation(n_components=2, random_state=42)
lda.fit(X)

for idx, topic in enumerate(lda.components_):
    print(f"Topic {idx}:")
    print([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-5:]])
```

### 4.2 Sentiment Analysis

‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‡πÄ‡∏ä‡πà‡∏ô ‡∏ö‡∏ß‡∏Å ‡∏Å‡∏•‡∏≤‡∏á ‡∏•‡∏ö

```python
from textblob import TextBlob

texts = ["I love this product!", "This is the worst experience ever."]
for t in texts:
    sentiment = TextBlob(t).sentiment.polarity
    print(f"'{t}' ‚Üí Sentiment Score: {sentiment}")
```

### 4.3 Clustering / Classification

‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡πÄ‡∏ä‡πà‡∏ô **K-means** ‡∏´‡∏£‡∏∑‡∏≠ **Na√Øve Bayes** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

texts = [
    "Artificial intelligence and machine learning",
    "Statistical analysis in data science",
    "Deep learning for NLP applications",
    "Predictive modeling and statistics"
]

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(texts)

kmeans = KMeans(n_clusters=2, random_state=42)
kmeans.fit(X)

for i, label in enumerate(kmeans.labels_):
    print(f"Text: {texts[i]} ‚Üí Cluster: {label}")
```

## 5. ‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î‡πÄ‡∏ä‡∏¥‡∏á‡∏ó‡∏§‡∏©‡∏é‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á

| ‡∏™‡∏≤‡∏Ç‡∏≤‡∏ß‡∏¥‡∏ä‡∏≤                              | ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢                                                                            |
| ------------------------------------- | ----------------------------------------------------------------------------------- |
| **Corpus Linguistics**                | ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏†‡∏≤‡∏©‡∏≤‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ñ‡∏•‡∏±‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥                  |
| **Information Retrieval (IR)**        | ‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏°‡∏≤‡∏Å                                 |
| **Natural Language Processing (NLP)** | ‡∏£‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÄ‡∏ä‡∏¥‡∏á‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì ‡πÄ‡∏ä‡πà‡∏ô ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ |

## 6. ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á

| ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå                      | ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢                                             |
| -------------------------------- | ---------------------------------------------------- |
| ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÇ‡∏û‡∏™‡∏ï‡πå‡πÉ‡∏ô‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏±‡∏á‡∏Ñ‡∏°‡∏≠‡∏≠‡∏ô‡πÑ‡∏•‡∏ô‡πå | ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ï‡πà‡∏≠‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ó‡∏≤‡∏á‡∏™‡∏±‡∏á‡∏Ñ‡∏°‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏±‡∏ê‡∏ö‡∏≤‡∏• |
| ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à            | ‡∏™‡∏Å‡∏±‡∏î‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÅ‡∏•‡∏∞‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏à‡∏≤‡∏Å‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ç‡πà‡∏≤‡∏ß                    |
| ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏£‡∏µ‡∏ß‡∏¥‡∏ß‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤             | ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏∂‡∏á‡∏û‡∏≠‡πÉ‡∏à‡∏Ç‡∏≠‡∏á‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤                          |
| ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£           | ‡πÉ‡∏ä‡πâ TF-IDF ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÅ‡∏•‡∏∞‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤             |

